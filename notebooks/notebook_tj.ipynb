{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    meta = pd.read_csv(path, index_col=False)\n",
    "    return meta\n",
    "\n",
    "# meta = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta  = read_file('../data/Aug2.0_Meta_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data/Aug2.0/'\n",
    "file_path = '../data/new_augmented.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['new_paths'] = root + meta['image_id']+'.jpg'\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = meta.iloc[:,[0,1,3,4,5,6,7]], meta.iloc[:,2]\n",
    "\n",
    "# train/test Split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "print(f\"train: {len(xtrain)}, test: {len(xtest)} \")\n",
    "\n",
    "# Concat X/y for train/test\n",
    "xtrain.insert(loc = 2, column = 'dx', value = ytrain)\n",
    "xtest.insert(loc= 2 , column = 'dx', value = ytest )\n",
    "\n",
    "# Writing to CSV\n",
    "xtest.to_csv('../data/Aug2.0_test.csv', index=False)\n",
    "xtrain.to_csv('../data/Aug2.0_train.csv', index=False)\n",
    "\n",
    "xtrain['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Paths(file):\n",
    "    meta = pd.read_csv(file)\n",
    "    Files_NotFound = []\n",
    "    for i, r in meta.iterrows():\n",
    "        if os.path.exists(meta.iloc[i, -1]):\n",
    "            continue\n",
    "        else:\n",
    "            Files_NotFound.append(meta.iloc[i, -1])\n",
    "    return Files_NotFound\n",
    "\n",
    "nf = Check_Paths('../data/Aug2.0_test.csv')\n",
    "len(nf)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/HAM10k/HAM10000_metadata.csv'\n",
    "def read_file(path):\n",
    "    meta = pd.read_csv(path, index_col=False)\n",
    "    return meta\n",
    "\n",
    "df = read_file(path)\n",
    "desc = df['dx'].value_counts()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, \n",
    "                        zoom_range= 0.1, horizontal_flip= True, rescale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dir = 'Aug2.0'\n",
    "basedir = '../data/HAM10k/HAM10000_images/'\n",
    "print(f\"Folder Exists..\") if os.path.exists('../data/'+aug_dir) else os.mkdir('../data/'+aug_dir)\n",
    "save_dir = '../data/'+aug_dir+'/'\n",
    "total_generated_images = 0\n",
    "for key in desc.keys():\n",
    "    print(key, desc[key])\n",
    "    ratio = int(desc['nv']/ desc[key])\n",
    "    print('Ratio to NV: ', ratio)\n",
    "    print('Augmenataions Needed/Image', ratio)\n",
    "    total_generated_images += ratio\n",
    "    all_images = df[df['dx'] == key]['image_id'].values\n",
    "    if(key == 'nv'):\n",
    "        continue\n",
    "    printonce= True\n",
    "    # iterate over all images augment them, save them and insert them in our metadata frame\n",
    "    for image_ in tqdm(all_images):\n",
    "        if(len(df[df['dx'] == key]) > len(df[df['dx'] == 'nv'])):\n",
    "            if printonce:\n",
    "                print(key, 'datapoints = ', len(df[df['dx'] == key]), 'reached above nv skipping more augmentations..')\n",
    "                printonce = False\n",
    "            continue\n",
    "    \n",
    "        image_path =  basedir + image_ + '.jpg'\n",
    "        image = load_img(image_path)\n",
    "        image = np.expand_dims(img_to_array(image), axis= 0)\n",
    "        generated = gen.flow(image)\n",
    "        row = df[df['image_id'] == image_]\n",
    "        dict_for_df = {\n",
    "            'lesion_id':row.lesion_id.values[0], 'image_id':row.image_id.values[0], \n",
    "            'dx':row.dx.values[0], 'dx_type':row.dx_type.values[0] ,\n",
    "            'age':row.age.values[0], 'sex':row.sex.values[0], 'localization':row.localization.values[0] \n",
    "        }\n",
    "       \n",
    "        for i in range(int(ratio)):\n",
    "            aug_image= next(generated).astype(np.uint8)\n",
    "            # save this image with an underscore\n",
    "            # add this to metadata dataframe\n",
    "            image_name= dict_for_df['image_id'] + '_' + str(i)\n",
    "            fname = image_name.split('_')\n",
    "            modify_name = fname[0]+'_'+fname[1]+'_'+fname[-1]\n",
    "            dict_for_df['image_id'] = modify_name\n",
    "            df = df.append(dict_for_df, ignore_index=True)\n",
    "            plt.imsave(save_dir + modify_name + '.jpg', aug_image[0])\n",
    "    \n",
    "print(f\"Total Images generated : {(total_generated_images)}\")\n",
    "print(f\"Consolidating All Images...\")\n",
    "\n",
    "# rename the images\n",
    "print(f\"Renaming files to desired format...\")\n",
    "for file in tqdm(os.listdir(save_dir)):\n",
    "    filename = file.split('_')\n",
    "    renamed = filename[0]+'_'+filename[1]+'_'+filename[-1]\n",
    "    os.rename(save_dir+file, save_dir+renamed)\n",
    "\n",
    "#  Copy the images\n",
    "for file in tqdm(os.listdir(basedir)):\n",
    "    shutil.copy(basedir+file, save_dir)\n",
    "    \n",
    "print(f\"Total Images: {len(os.listdir(save_dir))}\")\n",
    "# Modify and save the meta file\n",
    "try:\n",
    "    new_meta_file = f'{aug_dir}_Meta_all.csv'\n",
    "    # df['new_image_id'] = df['image_id'].split()\n",
    "    df.to_csv('../data/'+new_meta_file, index=False)\n",
    "    print(f\"New Meta File : {new_meta_file}\")\n",
    "except:\n",
    "    print(f\"FileError :: Error occured during meta file saving...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://github.com/pablonm3/gan_skin_cancer/blob/master/skin_cancer_data_gan.ipynb -o test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Renaming he files\n",
    "root = '../data/Aug2.0/'\n",
    "for file in os.listdir('../data/Aug2.0/'):\n",
    "    filename = file.split('_')\n",
    "    if len(filename) > 2:\n",
    "        renamed = filename[0]+'_'+filename[-1]\n",
    "    else:\n",
    "        renamed = file\n",
    "    os.rename(root + file, root + str(renamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/Aug2.0_Meta_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjan2021\\Documents\\GitHub\\SkinCancer\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import sys\n",
    "sys.path.append(path.abspath('../src/'))\n",
    "\n",
    "from dataset import SkinCancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "batch_size = 64\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "n_cpu=8\n",
    "latent_dim=100\n",
    "n_classes=7\n",
    "img_size=32\n",
    "channels=1\n",
    "sample_interval=400\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (aux_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=7, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if device == 'cuda':\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure data loader\n",
    "# os.makedirs(\"../data/mnist\", exist_ok=True)\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(\n",
    "#         \"../../data/mnist\",\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transforms.Compose(\n",
    "#             [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "#         ),\n",
    "#     ),\n",
    "#     batch_size=opt.batch_size,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# Skin Cancer Dataset\n",
    "data_dir = '../data/HAM10k/HAM10000_images/'\n",
    "train_data = SkinCancer(data_dir, '../data/minority_train.csv', transform=None)\n",
    "dataset_size = len(train_data)    \n",
    "test_data = SkinCancer(data_dir, '../data/minority_test.csv',transform=None)\n",
    "classes=np.unique(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if device == 'cuda' else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if device == 'cuda' else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,labels = next(iter(dataloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2656"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "165*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/166] [D loss: 0.835387, acc: 65%] [G loss: 2.860200]\n",
      "[Epoch 0/5] [Batch 1/166] [D loss: 4.439139, acc: 28%] [G loss: 2.220542]\n",
      "[Epoch 0/5] [Batch 2/166] [D loss: 4.556452, acc: 25%] [G loss: 1.368933]\n",
      "[Epoch 0/5] [Batch 3/166] [D loss: 3.681585, acc: 34%] [G loss: 2.087445]\n",
      "[Epoch 0/5] [Batch 4/166] [D loss: 3.828504, acc: 28%] [G loss: 1.206982]\n",
      "[Epoch 0/5] [Batch 5/166] [D loss: 2.969157, acc: 43%] [G loss: 1.233605]\n",
      "[Epoch 0/5] [Batch 6/166] [D loss: 3.227779, acc: 37%] [G loss: 1.267979]\n",
      "[Epoch 0/5] [Batch 7/166] [D loss: 3.412123, acc: 50%] [G loss: 1.790952]\n",
      "[Epoch 0/5] [Batch 8/166] [D loss: 3.317733, acc: 34%] [G loss: 0.994611]\n",
      "[Epoch 0/5] [Batch 9/166] [D loss: 3.657559, acc: 37%] [G loss: 1.188053]\n",
      "[Epoch 0/5] [Batch 10/166] [D loss: 3.330030, acc: 40%] [G loss: 1.999264]\n",
      "[Epoch 0/5] [Batch 11/166] [D loss: 2.563823, acc: 31%] [G loss: 1.090088]\n",
      "[Epoch 0/5] [Batch 12/166] [D loss: 2.558918, acc: 43%] [G loss: 1.368822]\n",
      "[Epoch 0/5] [Batch 13/166] [D loss: 3.022714, acc: 28%] [G loss: 1.402249]\n",
      "[Epoch 0/5] [Batch 14/166] [D loss: 3.489597, acc: 34%] [G loss: 1.088381]\n",
      "[Epoch 0/5] [Batch 15/166] [D loss: 2.438126, acc: 50%] [G loss: 1.157029]\n",
      "[Epoch 0/5] [Batch 16/166] [D loss: 2.497590, acc: 37%] [G loss: 1.393241]\n",
      "[Epoch 0/5] [Batch 17/166] [D loss: 2.153919, acc: 28%] [G loss: 1.366264]\n",
      "[Epoch 0/5] [Batch 18/166] [D loss: 2.179566, acc: 40%] [G loss: 1.761684]\n",
      "[Epoch 0/5] [Batch 19/166] [D loss: 2.169580, acc: 46%] [G loss: 2.032589]\n",
      "[Epoch 0/5] [Batch 20/166] [D loss: 2.512104, acc: 31%] [G loss: 1.330548]\n",
      "[Epoch 0/5] [Batch 21/166] [D loss: 1.881145, acc: 34%] [G loss: 1.777975]\n",
      "[Epoch 0/5] [Batch 22/166] [D loss: 2.301259, acc: 25%] [G loss: 1.264465]\n",
      "[Epoch 0/5] [Batch 23/166] [D loss: 1.883556, acc: 34%] [G loss: 1.223335]\n",
      "[Epoch 0/5] [Batch 24/166] [D loss: 3.296047, acc: 34%] [G loss: 1.094455]\n",
      "[Epoch 0/5] [Batch 25/166] [D loss: 2.442462, acc: 34%] [G loss: 1.072131]\n",
      "[Epoch 0/5] [Batch 26/166] [D loss: 2.000238, acc: 46%] [G loss: 1.461040]\n",
      "[Epoch 0/5] [Batch 27/166] [D loss: 1.697838, acc: 40%] [G loss: 1.107378]\n",
      "[Epoch 0/5] [Batch 28/166] [D loss: 1.939206, acc: 43%] [G loss: 2.220085]\n",
      "[Epoch 0/5] [Batch 29/166] [D loss: 2.169718, acc: 21%] [G loss: 1.285547]\n",
      "[Epoch 0/5] [Batch 30/166] [D loss: 2.318376, acc: 40%] [G loss: 2.692780]\n",
      "[Epoch 0/5] [Batch 31/166] [D loss: 2.484593, acc: 31%] [G loss: 1.403436]\n",
      "[Epoch 0/5] [Batch 32/166] [D loss: 1.457865, acc: 28%] [G loss: 1.174313]\n",
      "[Epoch 0/5] [Batch 33/166] [D loss: 1.998727, acc: 34%] [G loss: 1.391704]\n",
      "[Epoch 0/5] [Batch 34/166] [D loss: 2.686751, acc: 34%] [G loss: 1.237308]\n",
      "[Epoch 0/5] [Batch 35/166] [D loss: 2.083242, acc: 21%] [G loss: 1.374048]\n",
      "[Epoch 0/5] [Batch 36/166] [D loss: 2.507819, acc: 34%] [G loss: 1.241189]\n",
      "[Epoch 0/5] [Batch 37/166] [D loss: 2.394925, acc: 31%] [G loss: 1.433493]\n",
      "[Epoch 0/5] [Batch 38/166] [D loss: 2.205069, acc: 34%] [G loss: 1.220930]\n",
      "[Epoch 0/5] [Batch 39/166] [D loss: 1.728190, acc: 28%] [G loss: 1.522901]\n",
      "[Epoch 0/5] [Batch 40/166] [D loss: 2.007202, acc: 18%] [G loss: 2.334268]\n",
      "[Epoch 0/5] [Batch 41/166] [D loss: 2.254761, acc: 40%] [G loss: 1.073429]\n",
      "[Epoch 0/5] [Batch 42/166] [D loss: 2.226542, acc: 31%] [G loss: 1.323884]\n",
      "[Epoch 0/5] [Batch 43/166] [D loss: 2.171464, acc: 31%] [G loss: 1.353970]\n",
      "[Epoch 0/5] [Batch 44/166] [D loss: 1.881380, acc: 31%] [G loss: 1.637556]\n",
      "[Epoch 0/5] [Batch 45/166] [D loss: 1.587980, acc: 43%] [G loss: 1.495656]\n",
      "[Epoch 0/5] [Batch 46/166] [D loss: 2.055199, acc: 43%] [G loss: 1.831374]\n",
      "[Epoch 0/5] [Batch 47/166] [D loss: 2.268462, acc: 40%] [G loss: 1.555339]\n",
      "[Epoch 0/5] [Batch 48/166] [D loss: 1.769220, acc: 43%] [G loss: 1.262776]\n",
      "[Epoch 0/5] [Batch 49/166] [D loss: 1.883731, acc: 34%] [G loss: 1.046546]\n",
      "[Epoch 0/5] [Batch 50/166] [D loss: 2.636589, acc: 18%] [G loss: 1.755217]\n",
      "[Epoch 0/5] [Batch 51/166] [D loss: 2.130773, acc: 37%] [G loss: 1.498244]\n",
      "[Epoch 0/5] [Batch 52/166] [D loss: 1.834490, acc: 28%] [G loss: 1.263564]\n",
      "[Epoch 0/5] [Batch 53/166] [D loss: 1.617879, acc: 21%] [G loss: 1.376589]\n",
      "[Epoch 0/5] [Batch 54/166] [D loss: 1.774391, acc: 43%] [G loss: 1.731631]\n",
      "[Epoch 0/5] [Batch 55/166] [D loss: 1.559066, acc: 50%] [G loss: 1.246418]\n",
      "[Epoch 0/5] [Batch 56/166] [D loss: 2.091213, acc: 34%] [G loss: 1.413802]\n",
      "[Epoch 0/5] [Batch 57/166] [D loss: 2.302779, acc: 18%] [G loss: 1.514714]\n",
      "[Epoch 0/5] [Batch 58/166] [D loss: 1.951994, acc: 25%] [G loss: 1.453267]\n",
      "[Epoch 0/5] [Batch 59/166] [D loss: 1.799591, acc: 34%] [G loss: 1.297137]\n",
      "[Epoch 0/5] [Batch 60/166] [D loss: 1.859707, acc: 15%] [G loss: 2.355479]\n",
      "[Epoch 0/5] [Batch 61/166] [D loss: 1.769783, acc: 50%] [G loss: 1.626529]\n",
      "[Epoch 0/5] [Batch 62/166] [D loss: 1.573791, acc: 25%] [G loss: 2.215315]\n",
      "[Epoch 0/5] [Batch 63/166] [D loss: 2.370824, acc: 40%] [G loss: 1.204260]\n",
      "[Epoch 0/5] [Batch 64/166] [D loss: 1.867256, acc: 28%] [G loss: 1.588931]\n",
      "[Epoch 0/5] [Batch 65/166] [D loss: 1.709716, acc: 40%] [G loss: 1.628412]\n",
      "[Epoch 0/5] [Batch 66/166] [D loss: 1.828717, acc: 37%] [G loss: 1.950887]\n",
      "[Epoch 0/5] [Batch 67/166] [D loss: 1.722522, acc: 31%] [G loss: 1.398753]\n",
      "[Epoch 0/5] [Batch 68/166] [D loss: 1.988556, acc: 34%] [G loss: 1.291864]\n",
      "[Epoch 0/5] [Batch 69/166] [D loss: 1.867524, acc: 28%] [G loss: 1.260066]\n",
      "[Epoch 0/5] [Batch 70/166] [D loss: 1.361508, acc: 43%] [G loss: 1.631472]\n",
      "[Epoch 0/5] [Batch 71/166] [D loss: 1.704066, acc: 40%] [G loss: 1.178397]\n",
      "[Epoch 0/5] [Batch 72/166] [D loss: 2.165728, acc: 28%] [G loss: 1.447622]\n",
      "[Epoch 0/5] [Batch 73/166] [D loss: 1.529305, acc: 46%] [G loss: 1.689817]\n",
      "[Epoch 0/5] [Batch 74/166] [D loss: 1.659652, acc: 34%] [G loss: 1.810253]\n",
      "[Epoch 0/5] [Batch 75/166] [D loss: 1.854265, acc: 34%] [G loss: 2.109625]\n",
      "[Epoch 0/5] [Batch 76/166] [D loss: 1.714953, acc: 31%] [G loss: 1.266482]\n",
      "[Epoch 0/5] [Batch 77/166] [D loss: 1.648252, acc: 34%] [G loss: 1.736446]\n",
      "[Epoch 0/5] [Batch 78/166] [D loss: 1.797060, acc: 25%] [G loss: 1.174223]\n",
      "[Epoch 0/5] [Batch 79/166] [D loss: 1.855847, acc: 43%] [G loss: 1.395888]\n",
      "[Epoch 0/5] [Batch 80/166] [D loss: 1.847589, acc: 25%] [G loss: 1.258387]\n",
      "[Epoch 0/5] [Batch 81/166] [D loss: 1.719917, acc: 34%] [G loss: 1.302589]\n",
      "[Epoch 0/5] [Batch 82/166] [D loss: 1.384229, acc: 31%] [G loss: 1.546290]\n",
      "[Epoch 0/5] [Batch 83/166] [D loss: 2.048497, acc: 31%] [G loss: 1.168395]\n",
      "[Epoch 0/5] [Batch 84/166] [D loss: 1.696361, acc: 25%] [G loss: 1.624327]\n",
      "[Epoch 0/5] [Batch 85/166] [D loss: 1.733120, acc: 28%] [G loss: 1.417837]\n",
      "[Epoch 0/5] [Batch 86/166] [D loss: 1.596056, acc: 34%] [G loss: 1.205047]\n",
      "[Epoch 0/5] [Batch 87/166] [D loss: 1.655220, acc: 37%] [G loss: 1.452132]\n",
      "[Epoch 0/5] [Batch 88/166] [D loss: 1.766580, acc: 34%] [G loss: 1.497269]\n",
      "[Epoch 0/5] [Batch 89/166] [D loss: 1.635164, acc: 25%] [G loss: 1.365301]\n",
      "[Epoch 0/5] [Batch 90/166] [D loss: 1.576810, acc: 28%] [G loss: 1.262782]\n",
      "[Epoch 0/5] [Batch 91/166] [D loss: 1.626883, acc: 21%] [G loss: 1.484963]\n",
      "[Epoch 0/5] [Batch 92/166] [D loss: 1.543309, acc: 34%] [G loss: 1.459854]\n",
      "[Epoch 0/5] [Batch 93/166] [D loss: 1.386290, acc: 37%] [G loss: 1.304350]\n",
      "[Epoch 0/5] [Batch 94/166] [D loss: 1.258777, acc: 34%] [G loss: 1.021726]\n",
      "[Epoch 0/5] [Batch 95/166] [D loss: 1.384575, acc: 56%] [G loss: 1.357746]\n",
      "[Epoch 0/5] [Batch 96/166] [D loss: 1.167352, acc: 34%] [G loss: 1.631765]\n",
      "[Epoch 0/5] [Batch 97/166] [D loss: 1.106213, acc: 37%] [G loss: 1.462029]\n",
      "[Epoch 0/5] [Batch 98/166] [D loss: 1.632416, acc: 37%] [G loss: 1.266961]\n",
      "[Epoch 0/5] [Batch 99/166] [D loss: 1.845008, acc: 21%] [G loss: 1.429388]\n",
      "[Epoch 0/5] [Batch 100/166] [D loss: 1.478951, acc: 40%] [G loss: 1.471149]\n",
      "[Epoch 0/5] [Batch 101/166] [D loss: 1.431073, acc: 50%] [G loss: 1.146980]\n",
      "[Epoch 0/5] [Batch 102/166] [D loss: 1.484003, acc: 25%] [G loss: 1.759040]\n",
      "[Epoch 0/5] [Batch 103/166] [D loss: 1.746001, acc: 25%] [G loss: 1.324196]\n",
      "[Epoch 0/5] [Batch 104/166] [D loss: 1.435860, acc: 56%] [G loss: 1.330899]\n",
      "[Epoch 0/5] [Batch 105/166] [D loss: 1.515929, acc: 34%] [G loss: 1.586318]\n",
      "[Epoch 0/5] [Batch 106/166] [D loss: 1.699249, acc: 43%] [G loss: 1.089548]\n",
      "[Epoch 0/5] [Batch 107/166] [D loss: 1.922209, acc: 37%] [G loss: 1.240166]\n",
      "[Epoch 0/5] [Batch 108/166] [D loss: 1.665742, acc: 46%] [G loss: 1.677566]\n",
      "[Epoch 0/5] [Batch 109/166] [D loss: 1.431917, acc: 46%] [G loss: 1.566480]\n",
      "[Epoch 0/5] [Batch 110/166] [D loss: 1.758831, acc: 31%] [G loss: 1.494637]\n",
      "[Epoch 0/5] [Batch 111/166] [D loss: 1.385750, acc: 18%] [G loss: 1.773090]\n",
      "[Epoch 0/5] [Batch 112/166] [D loss: 1.699793, acc: 37%] [G loss: 1.382990]\n",
      "[Epoch 0/5] [Batch 113/166] [D loss: 1.611539, acc: 37%] [G loss: 1.079782]\n",
      "[Epoch 0/5] [Batch 114/166] [D loss: 2.027717, acc: 40%] [G loss: 1.088005]\n",
      "[Epoch 0/5] [Batch 115/166] [D loss: 1.415341, acc: 46%] [G loss: 1.207847]\n",
      "[Epoch 0/5] [Batch 116/166] [D loss: 1.368996, acc: 31%] [G loss: 1.333951]\n",
      "[Epoch 0/5] [Batch 117/166] [D loss: 1.736046, acc: 28%] [G loss: 1.587385]\n",
      "[Epoch 0/5] [Batch 118/166] [D loss: 1.469766, acc: 28%] [G loss: 1.272899]\n",
      "[Epoch 0/5] [Batch 119/166] [D loss: 1.527699, acc: 28%] [G loss: 1.225199]\n",
      "[Epoch 0/5] [Batch 120/166] [D loss: 1.520870, acc: 40%] [G loss: 1.070259]\n",
      "[Epoch 0/5] [Batch 121/166] [D loss: 1.707144, acc: 43%] [G loss: 1.128527]\n",
      "[Epoch 0/5] [Batch 122/166] [D loss: 1.597188, acc: 31%] [G loss: 1.617971]\n",
      "[Epoch 0/5] [Batch 123/166] [D loss: 1.292106, acc: 34%] [G loss: 1.244032]\n",
      "[Epoch 0/5] [Batch 124/166] [D loss: 1.246866, acc: 43%] [G loss: 1.168747]\n",
      "[Epoch 0/5] [Batch 125/166] [D loss: 1.349682, acc: 25%] [G loss: 1.412814]\n",
      "[Epoch 0/5] [Batch 126/166] [D loss: 1.447321, acc: 46%] [G loss: 1.426552]\n",
      "[Epoch 0/5] [Batch 127/166] [D loss: 1.770635, acc: 18%] [G loss: 1.272624]\n",
      "[Epoch 0/5] [Batch 128/166] [D loss: 1.443504, acc: 28%] [G loss: 1.534593]\n",
      "[Epoch 0/5] [Batch 129/166] [D loss: 1.789491, acc: 40%] [G loss: 1.426059]\n",
      "[Epoch 0/5] [Batch 130/166] [D loss: 1.793654, acc: 25%] [G loss: 1.254741]\n",
      "[Epoch 0/5] [Batch 131/166] [D loss: 1.253466, acc: 37%] [G loss: 1.248369]\n",
      "[Epoch 0/5] [Batch 132/166] [D loss: 1.655813, acc: 43%] [G loss: 1.412411]\n",
      "[Epoch 0/5] [Batch 133/166] [D loss: 1.403720, acc: 31%] [G loss: 1.448393]\n",
      "[Epoch 0/5] [Batch 134/166] [D loss: 1.435624, acc: 34%] [G loss: 1.459312]\n",
      "[Epoch 0/5] [Batch 135/166] [D loss: 1.768009, acc: 21%] [G loss: 1.578624]\n",
      "[Epoch 0/5] [Batch 136/166] [D loss: 1.355276, acc: 46%] [G loss: 1.500722]\n",
      "[Epoch 0/5] [Batch 137/166] [D loss: 1.668694, acc: 28%] [G loss: 1.413628]\n",
      "[Epoch 0/5] [Batch 138/166] [D loss: 1.648649, acc: 40%] [G loss: 1.374686]\n",
      "[Epoch 0/5] [Batch 139/166] [D loss: 1.570375, acc: 28%] [G loss: 1.247724]\n",
      "[Epoch 0/5] [Batch 140/166] [D loss: 1.136699, acc: 53%] [G loss: 1.449760]\n",
      "[Epoch 0/5] [Batch 141/166] [D loss: 1.409240, acc: 31%] [G loss: 1.377913]\n",
      "[Epoch 0/5] [Batch 142/166] [D loss: 1.366509, acc: 21%] [G loss: 1.327633]\n",
      "[Epoch 0/5] [Batch 143/166] [D loss: 1.405019, acc: 21%] [G loss: 1.360210]\n",
      "[Epoch 0/5] [Batch 144/166] [D loss: 1.438560, acc: 34%] [G loss: 1.328555]\n",
      "[Epoch 0/5] [Batch 145/166] [D loss: 1.538786, acc: 46%] [G loss: 1.644681]\n",
      "[Epoch 0/5] [Batch 146/166] [D loss: 1.889278, acc: 21%] [G loss: 1.530477]\n",
      "[Epoch 0/5] [Batch 147/166] [D loss: 1.311923, acc: 37%] [G loss: 1.283275]\n",
      "[Epoch 0/5] [Batch 148/166] [D loss: 1.509193, acc: 25%] [G loss: 1.749345]\n",
      "[Epoch 0/5] [Batch 149/166] [D loss: 1.795101, acc: 31%] [G loss: 1.494081]\n",
      "[Epoch 0/5] [Batch 150/166] [D loss: 1.579321, acc: 43%] [G loss: 1.662425]\n",
      "[Epoch 0/5] [Batch 151/166] [D loss: 1.373492, acc: 37%] [G loss: 1.414320]\n",
      "[Epoch 0/5] [Batch 152/166] [D loss: 1.429377, acc: 31%] [G loss: 1.751968]\n",
      "[Epoch 0/5] [Batch 153/166] [D loss: 1.236445, acc: 53%] [G loss: 1.566781]\n",
      "[Epoch 0/5] [Batch 154/166] [D loss: 1.168668, acc: 37%] [G loss: 1.314156]\n",
      "[Epoch 0/5] [Batch 155/166] [D loss: 1.286989, acc: 37%] [G loss: 1.402581]\n",
      "[Epoch 0/5] [Batch 156/166] [D loss: 1.598758, acc: 46%] [G loss: 1.552355]\n",
      "[Epoch 0/5] [Batch 157/166] [D loss: 1.742202, acc: 21%] [G loss: 1.395183]\n",
      "[Epoch 0/5] [Batch 158/166] [D loss: 1.518671, acc: 40%] [G loss: 1.080423]\n",
      "[Epoch 0/5] [Batch 159/166] [D loss: 1.572890, acc: 53%] [G loss: 1.397462]\n",
      "[Epoch 0/5] [Batch 160/166] [D loss: 1.718151, acc: 28%] [G loss: 1.501848]\n",
      "[Epoch 0/5] [Batch 161/166] [D loss: 1.455442, acc: 37%] [G loss: 1.659238]\n",
      "[Epoch 0/5] [Batch 162/166] [D loss: 1.495425, acc: 37%] [G loss: 1.342486]\n",
      "[Epoch 0/5] [Batch 163/166] [D loss: 1.714817, acc: 37%] [G loss: 1.292569]\n",
      "[Epoch 0/5] [Batch 164/166] [D loss: 1.456300, acc: 18%] [G loss: 1.596384]\n",
      "[Epoch 1/5] [Batch 0/166] [D loss: 1.281531, acc: 50%] [G loss: 1.772758]\n",
      "[Epoch 1/5] [Batch 1/166] [D loss: 1.538776, acc: 40%] [G loss: 1.138687]\n",
      "[Epoch 1/5] [Batch 2/166] [D loss: 1.712600, acc: 40%] [G loss: 1.278476]\n",
      "[Epoch 1/5] [Batch 3/166] [D loss: 1.178483, acc: 46%] [G loss: 1.687238]\n",
      "[Epoch 1/5] [Batch 4/166] [D loss: 1.493581, acc: 34%] [G loss: 1.427284]\n",
      "[Epoch 1/5] [Batch 5/166] [D loss: 1.494544, acc: 18%] [G loss: 1.796194]\n",
      "[Epoch 1/5] [Batch 6/166] [D loss: 1.451704, acc: 37%] [G loss: 1.418115]\n",
      "[Epoch 1/5] [Batch 7/166] [D loss: 1.172010, acc: 43%] [G loss: 1.629233]\n",
      "[Epoch 1/5] [Batch 8/166] [D loss: 1.255245, acc: 43%] [G loss: 1.412990]\n",
      "[Epoch 1/5] [Batch 9/166] [D loss: 1.714538, acc: 21%] [G loss: 1.179930]\n",
      "[Epoch 1/5] [Batch 10/166] [D loss: 1.378388, acc: 46%] [G loss: 1.512890]\n",
      "[Epoch 1/5] [Batch 11/166] [D loss: 1.655712, acc: 37%] [G loss: 1.173039]\n",
      "[Epoch 1/5] [Batch 12/166] [D loss: 1.292331, acc: 28%] [G loss: 1.477141]\n",
      "[Epoch 1/5] [Batch 13/166] [D loss: 1.436839, acc: 34%] [G loss: 1.569968]\n",
      "[Epoch 1/5] [Batch 14/166] [D loss: 1.286803, acc: 34%] [G loss: 1.576484]\n",
      "[Epoch 1/5] [Batch 15/166] [D loss: 1.372031, acc: 43%] [G loss: 1.448906]\n",
      "[Epoch 1/5] [Batch 16/166] [D loss: 1.426116, acc: 37%] [G loss: 1.552046]\n",
      "[Epoch 1/5] [Batch 17/166] [D loss: 1.462203, acc: 28%] [G loss: 1.692593]\n",
      "[Epoch 1/5] [Batch 18/166] [D loss: 1.246549, acc: 46%] [G loss: 1.350601]\n",
      "[Epoch 1/5] [Batch 19/166] [D loss: 1.429816, acc: 46%] [G loss: 1.314991]\n",
      "[Epoch 1/5] [Batch 20/166] [D loss: 1.353021, acc: 53%] [G loss: 1.407333]\n",
      "[Epoch 1/5] [Batch 21/166] [D loss: 1.253322, acc: 50%] [G loss: 1.624448]\n",
      "[Epoch 1/5] [Batch 22/166] [D loss: 1.297026, acc: 37%] [G loss: 1.330534]\n",
      "[Epoch 1/5] [Batch 23/166] [D loss: 1.539578, acc: 34%] [G loss: 1.453802]\n",
      "[Epoch 1/5] [Batch 24/166] [D loss: 1.423755, acc: 25%] [G loss: 1.636882]\n",
      "[Epoch 1/5] [Batch 25/166] [D loss: 1.338811, acc: 34%] [G loss: 1.417383]\n",
      "[Epoch 1/5] [Batch 26/166] [D loss: 1.555451, acc: 43%] [G loss: 1.185919]\n",
      "[Epoch 1/5] [Batch 27/166] [D loss: 1.388946, acc: 37%] [G loss: 1.224422]\n",
      "[Epoch 1/5] [Batch 28/166] [D loss: 1.745449, acc: 50%] [G loss: 1.452973]\n",
      "[Epoch 1/5] [Batch 29/166] [D loss: 1.295617, acc: 37%] [G loss: 1.688149]\n",
      "[Epoch 1/5] [Batch 30/166] [D loss: 1.485514, acc: 40%] [G loss: 1.377202]\n",
      "[Epoch 1/5] [Batch 31/166] [D loss: 1.264332, acc: 31%] [G loss: 1.789127]\n",
      "[Epoch 1/5] [Batch 32/166] [D loss: 1.400137, acc: 21%] [G loss: 1.334023]\n",
      "[Epoch 1/5] [Batch 33/166] [D loss: 1.275742, acc: 50%] [G loss: 1.397915]\n",
      "[Epoch 1/5] [Batch 34/166] [D loss: 1.709462, acc: 31%] [G loss: 1.471050]\n",
      "[Epoch 1/5] [Batch 35/166] [D loss: 1.607011, acc: 25%] [G loss: 1.622025]\n",
      "[Epoch 1/5] [Batch 36/166] [D loss: 1.231673, acc: 37%] [G loss: 1.642260]\n",
      "[Epoch 1/5] [Batch 37/166] [D loss: 1.705874, acc: 40%] [G loss: 1.299053]\n",
      "[Epoch 1/5] [Batch 38/166] [D loss: 1.507877, acc: 40%] [G loss: 1.477548]\n",
      "[Epoch 1/5] [Batch 39/166] [D loss: 1.642688, acc: 46%] [G loss: 1.421936]\n",
      "[Epoch 1/5] [Batch 40/166] [D loss: 1.368521, acc: 28%] [G loss: 1.669616]\n",
      "[Epoch 1/5] [Batch 41/166] [D loss: 1.346723, acc: 43%] [G loss: 1.687788]\n",
      "[Epoch 1/5] [Batch 42/166] [D loss: 1.673100, acc: 37%] [G loss: 1.184852]\n",
      "[Epoch 1/5] [Batch 43/166] [D loss: 1.448732, acc: 37%] [G loss: 1.377567]\n",
      "[Epoch 1/5] [Batch 44/166] [D loss: 1.479730, acc: 37%] [G loss: 1.642191]\n",
      "[Epoch 1/5] [Batch 45/166] [D loss: 1.145322, acc: 59%] [G loss: 1.237331]\n",
      "[Epoch 1/5] [Batch 46/166] [D loss: 1.445796, acc: 37%] [G loss: 1.342095]\n",
      "[Epoch 1/5] [Batch 47/166] [D loss: 1.498845, acc: 28%] [G loss: 1.396708]\n",
      "[Epoch 1/5] [Batch 48/166] [D loss: 1.355134, acc: 40%] [G loss: 1.231301]\n",
      "[Epoch 1/5] [Batch 49/166] [D loss: 1.492050, acc: 34%] [G loss: 1.588894]\n",
      "[Epoch 1/5] [Batch 50/166] [D loss: 1.640874, acc: 25%] [G loss: 1.603265]\n",
      "[Epoch 1/5] [Batch 51/166] [D loss: 1.609805, acc: 28%] [G loss: 1.567135]\n",
      "[Epoch 1/5] [Batch 52/166] [D loss: 1.306739, acc: 34%] [G loss: 1.222744]\n",
      "[Epoch 1/5] [Batch 53/166] [D loss: 1.228637, acc: 31%] [G loss: 1.486558]\n",
      "[Epoch 1/5] [Batch 54/166] [D loss: 1.735325, acc: 37%] [G loss: 1.864794]\n",
      "[Epoch 1/5] [Batch 55/166] [D loss: 1.254838, acc: 40%] [G loss: 1.690774]\n",
      "[Epoch 1/5] [Batch 56/166] [D loss: 1.392690, acc: 31%] [G loss: 1.539684]\n",
      "[Epoch 1/5] [Batch 57/166] [D loss: 1.510088, acc: 31%] [G loss: 1.958951]\n",
      "[Epoch 1/5] [Batch 58/166] [D loss: 1.349656, acc: 28%] [G loss: 1.753853]\n",
      "[Epoch 1/5] [Batch 59/166] [D loss: 1.206954, acc: 43%] [G loss: 1.191326]\n",
      "[Epoch 1/5] [Batch 60/166] [D loss: 1.351837, acc: 34%] [G loss: 1.450690]\n",
      "[Epoch 1/5] [Batch 61/166] [D loss: 1.482567, acc: 43%] [G loss: 1.309342]\n",
      "[Epoch 1/5] [Batch 62/166] [D loss: 1.509969, acc: 25%] [G loss: 1.878534]\n",
      "[Epoch 1/5] [Batch 63/166] [D loss: 1.436742, acc: 28%] [G loss: 1.191211]\n",
      "[Epoch 1/5] [Batch 64/166] [D loss: 1.277673, acc: 25%] [G loss: 1.459168]\n",
      "[Epoch 1/5] [Batch 65/166] [D loss: 1.737096, acc: 37%] [G loss: 1.245566]\n",
      "[Epoch 1/5] [Batch 66/166] [D loss: 1.343081, acc: 31%] [G loss: 1.373783]\n",
      "[Epoch 1/5] [Batch 67/166] [D loss: 1.262621, acc: 46%] [G loss: 1.655398]\n",
      "[Epoch 1/5] [Batch 68/166] [D loss: 1.629661, acc: 50%] [G loss: 1.257289]\n",
      "[Epoch 1/5] [Batch 69/166] [D loss: 1.242727, acc: 34%] [G loss: 1.375208]\n",
      "[Epoch 1/5] [Batch 70/166] [D loss: 1.304634, acc: 37%] [G loss: 1.786948]\n",
      "[Epoch 1/5] [Batch 71/166] [D loss: 1.510176, acc: 37%] [G loss: 1.320018]\n",
      "[Epoch 1/5] [Batch 72/166] [D loss: 1.784990, acc: 43%] [G loss: 1.389287]\n",
      "[Epoch 1/5] [Batch 73/166] [D loss: 1.393015, acc: 40%] [G loss: 1.490001]\n",
      "[Epoch 1/5] [Batch 74/166] [D loss: 1.275402, acc: 46%] [G loss: 1.456775]\n",
      "[Epoch 1/5] [Batch 75/166] [D loss: 1.632728, acc: 31%] [G loss: 1.466080]\n",
      "[Epoch 1/5] [Batch 76/166] [D loss: 1.537511, acc: 43%] [G loss: 1.278389]\n",
      "[Epoch 1/5] [Batch 77/166] [D loss: 1.191252, acc: 50%] [G loss: 1.380781]\n",
      "[Epoch 1/5] [Batch 78/166] [D loss: 1.411817, acc: 43%] [G loss: 1.224218]\n",
      "[Epoch 1/5] [Batch 79/166] [D loss: 1.221891, acc: 43%] [G loss: 1.323034]\n",
      "[Epoch 1/5] [Batch 80/166] [D loss: 1.485482, acc: 46%] [G loss: 1.415487]\n",
      "[Epoch 1/5] [Batch 81/166] [D loss: 1.387836, acc: 34%] [G loss: 1.510451]\n",
      "[Epoch 1/5] [Batch 82/166] [D loss: 1.321862, acc: 28%] [G loss: 1.295041]\n",
      "[Epoch 1/5] [Batch 83/166] [D loss: 1.162859, acc: 43%] [G loss: 1.387439]\n",
      "[Epoch 1/5] [Batch 84/166] [D loss: 1.539173, acc: 43%] [G loss: 1.360763]\n",
      "[Epoch 1/5] [Batch 85/166] [D loss: 1.268011, acc: 31%] [G loss: 1.541184]\n",
      "[Epoch 1/5] [Batch 86/166] [D loss: 1.463341, acc: 21%] [G loss: 1.412246]\n",
      "[Epoch 1/5] [Batch 87/166] [D loss: 1.269613, acc: 37%] [G loss: 1.166768]\n",
      "[Epoch 1/5] [Batch 88/166] [D loss: 1.417769, acc: 37%] [G loss: 1.483823]\n",
      "[Epoch 1/5] [Batch 89/166] [D loss: 1.506273, acc: 21%] [G loss: 1.725918]\n",
      "[Epoch 1/5] [Batch 90/166] [D loss: 1.421306, acc: 31%] [G loss: 1.595804]\n",
      "[Epoch 1/5] [Batch 91/166] [D loss: 1.578809, acc: 37%] [G loss: 1.413651]\n",
      "[Epoch 1/5] [Batch 92/166] [D loss: 1.409159, acc: 28%] [G loss: 1.086747]\n",
      "[Epoch 1/5] [Batch 93/166] [D loss: 1.426432, acc: 43%] [G loss: 1.435993]\n",
      "[Epoch 1/5] [Batch 94/166] [D loss: 1.306214, acc: 46%] [G loss: 1.218404]\n",
      "[Epoch 1/5] [Batch 95/166] [D loss: 1.238242, acc: 56%] [G loss: 1.575549]\n",
      "[Epoch 1/5] [Batch 96/166] [D loss: 1.402431, acc: 28%] [G loss: 2.104130]\n",
      "[Epoch 1/5] [Batch 97/166] [D loss: 1.115952, acc: 40%] [G loss: 2.042292]\n",
      "[Epoch 1/5] [Batch 98/166] [D loss: 1.515952, acc: 43%] [G loss: 1.530761]\n",
      "[Epoch 1/5] [Batch 99/166] [D loss: 1.490576, acc: 31%] [G loss: 1.547222]\n",
      "[Epoch 1/5] [Batch 100/166] [D loss: 1.419024, acc: 34%] [G loss: 1.471433]\n",
      "[Epoch 1/5] [Batch 101/166] [D loss: 1.291116, acc: 37%] [G loss: 1.369491]\n",
      "[Epoch 1/5] [Batch 102/166] [D loss: 1.384785, acc: 46%] [G loss: 1.267344]\n",
      "[Epoch 1/5] [Batch 103/166] [D loss: 1.382098, acc: 25%] [G loss: 1.235656]\n",
      "[Epoch 1/5] [Batch 104/166] [D loss: 1.600592, acc: 46%] [G loss: 1.505983]\n",
      "[Epoch 1/5] [Batch 105/166] [D loss: 1.369629, acc: 46%] [G loss: 1.149944]\n",
      "[Epoch 1/5] [Batch 106/166] [D loss: 1.404600, acc: 53%] [G loss: 1.258573]\n",
      "[Epoch 1/5] [Batch 107/166] [D loss: 1.394047, acc: 37%] [G loss: 1.436828]\n",
      "[Epoch 1/5] [Batch 108/166] [D loss: 1.502910, acc: 34%] [G loss: 1.503984]\n",
      "[Epoch 1/5] [Batch 109/166] [D loss: 1.510634, acc: 43%] [G loss: 1.590007]\n",
      "[Epoch 1/5] [Batch 110/166] [D loss: 1.327246, acc: 37%] [G loss: 1.734456]\n",
      "[Epoch 1/5] [Batch 111/166] [D loss: 1.450544, acc: 40%] [G loss: 1.303944]\n",
      "[Epoch 1/5] [Batch 112/166] [D loss: 1.414144, acc: 31%] [G loss: 1.220768]\n",
      "[Epoch 1/5] [Batch 113/166] [D loss: 1.466618, acc: 28%] [G loss: 1.669347]\n",
      "[Epoch 1/5] [Batch 114/166] [D loss: 1.506818, acc: 34%] [G loss: 1.765556]\n",
      "[Epoch 1/5] [Batch 115/166] [D loss: 1.257176, acc: 43%] [G loss: 1.340763]\n",
      "[Epoch 1/5] [Batch 116/166] [D loss: 1.332908, acc: 46%] [G loss: 1.820810]\n",
      "[Epoch 1/5] [Batch 117/166] [D loss: 1.309373, acc: 37%] [G loss: 1.460356]\n",
      "[Epoch 1/5] [Batch 118/166] [D loss: 1.479955, acc: 18%] [G loss: 1.499357]\n",
      "[Epoch 1/5] [Batch 119/166] [D loss: 1.340834, acc: 25%] [G loss: 1.466472]\n",
      "[Epoch 1/5] [Batch 120/166] [D loss: 1.236268, acc: 43%] [G loss: 1.297662]\n",
      "[Epoch 1/5] [Batch 121/166] [D loss: 1.668179, acc: 34%] [G loss: 1.578296]\n",
      "[Epoch 1/5] [Batch 122/166] [D loss: 1.476628, acc: 40%] [G loss: 1.463768]\n",
      "[Epoch 1/5] [Batch 123/166] [D loss: 1.617378, acc: 43%] [G loss: 1.214415]\n",
      "[Epoch 1/5] [Batch 124/166] [D loss: 1.362550, acc: 50%] [G loss: 0.994537]\n",
      "[Epoch 1/5] [Batch 125/166] [D loss: 1.207918, acc: 43%] [G loss: 1.198321]\n",
      "[Epoch 1/5] [Batch 126/166] [D loss: 1.418226, acc: 28%] [G loss: 1.495182]\n",
      "[Epoch 1/5] [Batch 127/166] [D loss: 1.416828, acc: 21%] [G loss: 1.931479]\n",
      "[Epoch 1/5] [Batch 128/166] [D loss: 1.460922, acc: 59%] [G loss: 1.625716]\n",
      "[Epoch 1/5] [Batch 129/166] [D loss: 1.594713, acc: 31%] [G loss: 1.448279]\n",
      "[Epoch 1/5] [Batch 130/166] [D loss: 1.355413, acc: 28%] [G loss: 1.333288]\n",
      "[Epoch 1/5] [Batch 131/166] [D loss: 1.347522, acc: 25%] [G loss: 1.203170]\n",
      "[Epoch 1/5] [Batch 132/166] [D loss: 1.404016, acc: 34%] [G loss: 1.433277]\n",
      "[Epoch 1/5] [Batch 133/166] [D loss: 1.298555, acc: 25%] [G loss: 1.415929]\n",
      "[Epoch 1/5] [Batch 134/166] [D loss: 1.357266, acc: 40%] [G loss: 1.408312]\n",
      "[Epoch 1/5] [Batch 135/166] [D loss: 1.355357, acc: 21%] [G loss: 1.462519]\n",
      "[Epoch 1/5] [Batch 136/166] [D loss: 1.370229, acc: 40%] [G loss: 1.482459]\n",
      "[Epoch 1/5] [Batch 137/166] [D loss: 1.529900, acc: 28%] [G loss: 1.262415]\n",
      "[Epoch 1/5] [Batch 138/166] [D loss: 1.592253, acc: 37%] [G loss: 1.272086]\n",
      "[Epoch 1/5] [Batch 139/166] [D loss: 1.465123, acc: 37%] [G loss: 1.795685]\n",
      "[Epoch 1/5] [Batch 140/166] [D loss: 1.232404, acc: 53%] [G loss: 1.460461]\n",
      "[Epoch 1/5] [Batch 141/166] [D loss: 1.355964, acc: 34%] [G loss: 1.751931]\n",
      "[Epoch 1/5] [Batch 142/166] [D loss: 1.456436, acc: 25%] [G loss: 2.058203]\n",
      "[Epoch 1/5] [Batch 143/166] [D loss: 1.329308, acc: 34%] [G loss: 1.374337]\n",
      "[Epoch 1/5] [Batch 144/166] [D loss: 1.390678, acc: 31%] [G loss: 2.043567]\n",
      "[Epoch 1/5] [Batch 145/166] [D loss: 1.325542, acc: 37%] [G loss: 1.547564]\n",
      "[Epoch 1/5] [Batch 146/166] [D loss: 1.470842, acc: 31%] [G loss: 1.611818]\n",
      "[Epoch 1/5] [Batch 147/166] [D loss: 1.456504, acc: 34%] [G loss: 1.486180]\n",
      "[Epoch 1/5] [Batch 148/166] [D loss: 1.312241, acc: 40%] [G loss: 1.135634]\n",
      "[Epoch 1/5] [Batch 149/166] [D loss: 1.381709, acc: 53%] [G loss: 1.409022]\n",
      "[Epoch 1/5] [Batch 150/166] [D loss: 1.337789, acc: 46%] [G loss: 1.803227]\n",
      "[Epoch 1/5] [Batch 151/166] [D loss: 1.617515, acc: 40%] [G loss: 1.270834]\n",
      "[Epoch 1/5] [Batch 152/166] [D loss: 1.514293, acc: 15%] [G loss: 1.736358]\n",
      "[Epoch 1/5] [Batch 153/166] [D loss: 1.327018, acc: 40%] [G loss: 1.214688]\n",
      "[Epoch 1/5] [Batch 154/166] [D loss: 1.332328, acc: 34%] [G loss: 1.340667]\n",
      "[Epoch 1/5] [Batch 155/166] [D loss: 1.413345, acc: 34%] [G loss: 1.549859]\n",
      "[Epoch 1/5] [Batch 156/166] [D loss: 1.271112, acc: 31%] [G loss: 1.638681]\n",
      "[Epoch 1/5] [Batch 157/166] [D loss: 1.464471, acc: 34%] [G loss: 1.353209]\n",
      "[Epoch 1/5] [Batch 158/166] [D loss: 1.430600, acc: 46%] [G loss: 1.760168]\n",
      "[Epoch 1/5] [Batch 159/166] [D loss: 1.484646, acc: 43%] [G loss: 1.427187]\n",
      "[Epoch 1/5] [Batch 160/166] [D loss: 1.340418, acc: 31%] [G loss: 1.565778]\n",
      "[Epoch 1/5] [Batch 161/166] [D loss: 1.321009, acc: 50%] [G loss: 1.225839]\n",
      "[Epoch 1/5] [Batch 162/166] [D loss: 1.234513, acc: 25%] [G loss: 1.799340]\n",
      "[Epoch 1/5] [Batch 163/166] [D loss: 1.346229, acc: 34%] [G loss: 1.429644]\n",
      "[Epoch 1/5] [Batch 164/166] [D loss: 1.441073, acc: 50%] [G loss: 1.374721]\n",
      "[Epoch 2/5] [Batch 0/166] [D loss: 1.353446, acc: 56%] [G loss: 1.394534]\n",
      "[Epoch 2/5] [Batch 1/166] [D loss: 1.307077, acc: 43%] [G loss: 1.773368]\n",
      "[Epoch 2/5] [Batch 2/166] [D loss: 1.342577, acc: 21%] [G loss: 1.535166]\n",
      "[Epoch 2/5] [Batch 3/166] [D loss: 1.333852, acc: 40%] [G loss: 1.249587]\n",
      "[Epoch 2/5] [Batch 4/166] [D loss: 1.353954, acc: 53%] [G loss: 1.726842]\n",
      "[Epoch 2/5] [Batch 5/166] [D loss: 1.425946, acc: 37%] [G loss: 1.279976]\n",
      "[Epoch 2/5] [Batch 6/166] [D loss: 1.678638, acc: 40%] [G loss: 1.221723]\n",
      "[Epoch 2/5] [Batch 7/166] [D loss: 1.326403, acc: 40%] [G loss: 1.357283]\n",
      "[Epoch 2/5] [Batch 8/166] [D loss: 1.361994, acc: 43%] [G loss: 1.439960]\n",
      "[Epoch 2/5] [Batch 9/166] [D loss: 1.513854, acc: 43%] [G loss: 1.391044]\n",
      "[Epoch 2/5] [Batch 10/166] [D loss: 1.450922, acc: 43%] [G loss: 1.544982]\n",
      "[Epoch 2/5] [Batch 11/166] [D loss: 1.235784, acc: 40%] [G loss: 1.673276]\n",
      "[Epoch 2/5] [Batch 12/166] [D loss: 1.291086, acc: 37%] [G loss: 1.527471]\n",
      "[Epoch 2/5] [Batch 13/166] [D loss: 1.179640, acc: 43%] [G loss: 1.514141]\n",
      "[Epoch 2/5] [Batch 14/166] [D loss: 1.218832, acc: 46%] [G loss: 1.311856]\n",
      "[Epoch 2/5] [Batch 15/166] [D loss: 1.501331, acc: 37%] [G loss: 1.069402]\n",
      "[Epoch 2/5] [Batch 16/166] [D loss: 1.235080, acc: 40%] [G loss: 1.677487]\n",
      "[Epoch 2/5] [Batch 17/166] [D loss: 1.292780, acc: 43%] [G loss: 1.456540]\n",
      "[Epoch 2/5] [Batch 18/166] [D loss: 1.431878, acc: 43%] [G loss: 1.135858]\n",
      "[Epoch 2/5] [Batch 19/166] [D loss: 1.375909, acc: 43%] [G loss: 0.984892]\n",
      "[Epoch 2/5] [Batch 20/166] [D loss: 1.258604, acc: 40%] [G loss: 1.426721]\n",
      "[Epoch 2/5] [Batch 21/166] [D loss: 1.301608, acc: 56%] [G loss: 1.352933]\n",
      "[Epoch 2/5] [Batch 22/166] [D loss: 1.304876, acc: 28%] [G loss: 1.474017]\n",
      "[Epoch 2/5] [Batch 23/166] [D loss: 1.263527, acc: 40%] [G loss: 1.704088]\n",
      "[Epoch 2/5] [Batch 24/166] [D loss: 1.399555, acc: 46%] [G loss: 1.390966]\n",
      "[Epoch 2/5] [Batch 25/166] [D loss: 1.319756, acc: 43%] [G loss: 1.775423]\n",
      "[Epoch 2/5] [Batch 26/166] [D loss: 1.262780, acc: 28%] [G loss: 1.789976]\n",
      "[Epoch 2/5] [Batch 27/166] [D loss: 1.291908, acc: 34%] [G loss: 1.829177]\n",
      "[Epoch 2/5] [Batch 28/166] [D loss: 1.522832, acc: 31%] [G loss: 1.469757]\n",
      "[Epoch 2/5] [Batch 29/166] [D loss: 1.384304, acc: 34%] [G loss: 1.540112]\n",
      "[Epoch 2/5] [Batch 30/166] [D loss: 1.457993, acc: 37%] [G loss: 1.259430]\n",
      "[Epoch 2/5] [Batch 31/166] [D loss: 1.180112, acc: 43%] [G loss: 1.457843]\n",
      "[Epoch 2/5] [Batch 32/166] [D loss: 1.366361, acc: 31%] [G loss: 1.514540]\n",
      "[Epoch 2/5] [Batch 33/166] [D loss: 1.256872, acc: 37%] [G loss: 1.545399]\n",
      "[Epoch 2/5] [Batch 34/166] [D loss: 1.410095, acc: 28%] [G loss: 1.677223]\n",
      "[Epoch 2/5] [Batch 35/166] [D loss: 1.350431, acc: 53%] [G loss: 1.382244]\n",
      "[Epoch 2/5] [Batch 36/166] [D loss: 1.226481, acc: 28%] [G loss: 2.272801]\n",
      "[Epoch 2/5] [Batch 37/166] [D loss: 1.387126, acc: 43%] [G loss: 1.470168]\n",
      "[Epoch 2/5] [Batch 38/166] [D loss: 1.310961, acc: 34%] [G loss: 1.677418]\n",
      "[Epoch 2/5] [Batch 39/166] [D loss: 1.388623, acc: 46%] [G loss: 1.280692]\n",
      "[Epoch 2/5] [Batch 40/166] [D loss: 1.401124, acc: 37%] [G loss: 1.393769]\n",
      "[Epoch 2/5] [Batch 41/166] [D loss: 1.399027, acc: 34%] [G loss: 1.163274]\n",
      "[Epoch 2/5] [Batch 42/166] [D loss: 1.520407, acc: 31%] [G loss: 1.330495]\n",
      "[Epoch 2/5] [Batch 43/166] [D loss: 1.156778, acc: 46%] [G loss: 1.645611]\n",
      "[Epoch 2/5] [Batch 44/166] [D loss: 1.497979, acc: 34%] [G loss: 1.299888]\n",
      "[Epoch 2/5] [Batch 45/166] [D loss: 1.215338, acc: 46%] [G loss: 1.599813]\n",
      "[Epoch 2/5] [Batch 46/166] [D loss: 1.230604, acc: 43%] [G loss: 1.684222]\n",
      "[Epoch 2/5] [Batch 47/166] [D loss: 1.384208, acc: 37%] [G loss: 1.351790]\n",
      "[Epoch 2/5] [Batch 48/166] [D loss: 1.298879, acc: 46%] [G loss: 1.306257]\n",
      "[Epoch 2/5] [Batch 49/166] [D loss: 1.261373, acc: 25%] [G loss: 1.781159]\n",
      "[Epoch 2/5] [Batch 50/166] [D loss: 1.258837, acc: 37%] [G loss: 1.363175]\n",
      "[Epoch 2/5] [Batch 51/166] [D loss: 1.276222, acc: 31%] [G loss: 1.700418]\n",
      "[Epoch 2/5] [Batch 52/166] [D loss: 1.301711, acc: 28%] [G loss: 1.706539]\n",
      "[Epoch 2/5] [Batch 53/166] [D loss: 1.387415, acc: 37%] [G loss: 1.307367]\n",
      "[Epoch 2/5] [Batch 54/166] [D loss: 1.636444, acc: 37%] [G loss: 1.793889]\n",
      "[Epoch 2/5] [Batch 55/166] [D loss: 1.450246, acc: 37%] [G loss: 1.675039]\n",
      "[Epoch 2/5] [Batch 56/166] [D loss: 1.256161, acc: 25%] [G loss: 1.593366]\n",
      "[Epoch 2/5] [Batch 57/166] [D loss: 1.472006, acc: 18%] [G loss: 1.397361]\n",
      "[Epoch 2/5] [Batch 58/166] [D loss: 1.468300, acc: 37%] [G loss: 1.451447]\n",
      "[Epoch 2/5] [Batch 59/166] [D loss: 1.477129, acc: 40%] [G loss: 1.521199]\n",
      "[Epoch 2/5] [Batch 60/166] [D loss: 1.462949, acc: 18%] [G loss: 1.674831]\n",
      "[Epoch 2/5] [Batch 61/166] [D loss: 1.286425, acc: 34%] [G loss: 1.611092]\n",
      "[Epoch 2/5] [Batch 62/166] [D loss: 1.400109, acc: 40%] [G loss: 1.515561]\n",
      "[Epoch 2/5] [Batch 63/166] [D loss: 1.395585, acc: 37%] [G loss: 1.316767]\n",
      "[Epoch 2/5] [Batch 64/166] [D loss: 1.511531, acc: 40%] [G loss: 1.403898]\n",
      "[Epoch 2/5] [Batch 65/166] [D loss: 1.431999, acc: 28%] [G loss: 1.398741]\n",
      "[Epoch 2/5] [Batch 66/166] [D loss: 1.292146, acc: 43%] [G loss: 1.464996]\n",
      "[Epoch 2/5] [Batch 67/166] [D loss: 1.397624, acc: 46%] [G loss: 1.589203]\n",
      "[Epoch 2/5] [Batch 68/166] [D loss: 1.197633, acc: 46%] [G loss: 1.428728]\n",
      "[Epoch 2/5] [Batch 69/166] [D loss: 1.558096, acc: 34%] [G loss: 1.228408]\n",
      "[Epoch 2/5] [Batch 70/166] [D loss: 1.213731, acc: 50%] [G loss: 1.541208]\n",
      "[Epoch 2/5] [Batch 71/166] [D loss: 1.481024, acc: 50%] [G loss: 1.006552]\n",
      "[Epoch 2/5] [Batch 72/166] [D loss: 1.748454, acc: 56%] [G loss: 1.718438]\n",
      "[Epoch 2/5] [Batch 73/166] [D loss: 1.233751, acc: 21%] [G loss: 1.378754]\n",
      "[Epoch 2/5] [Batch 74/166] [D loss: 1.124909, acc: 53%] [G loss: 1.508180]\n",
      "[Epoch 2/5] [Batch 75/166] [D loss: 1.350095, acc: 46%] [G loss: 1.529889]\n",
      "[Epoch 2/5] [Batch 76/166] [D loss: 1.286487, acc: 37%] [G loss: 1.371871]\n",
      "[Epoch 2/5] [Batch 77/166] [D loss: 1.362660, acc: 40%] [G loss: 1.326312]\n",
      "[Epoch 2/5] [Batch 78/166] [D loss: 1.169714, acc: 43%] [G loss: 1.244571]\n",
      "[Epoch 2/5] [Batch 79/166] [D loss: 1.219532, acc: 56%] [G loss: 1.478587]\n",
      "[Epoch 2/5] [Batch 80/166] [D loss: 1.494819, acc: 25%] [G loss: 1.379795]\n",
      "[Epoch 2/5] [Batch 81/166] [D loss: 1.271528, acc: 37%] [G loss: 1.459375]\n",
      "[Epoch 2/5] [Batch 82/166] [D loss: 1.308987, acc: 43%] [G loss: 1.775391]\n",
      "[Epoch 2/5] [Batch 83/166] [D loss: 1.400437, acc: 28%] [G loss: 1.345339]\n",
      "[Epoch 2/5] [Batch 84/166] [D loss: 1.307858, acc: 53%] [G loss: 1.208466]\n",
      "[Epoch 2/5] [Batch 85/166] [D loss: 1.576585, acc: 28%] [G loss: 1.493613]\n",
      "[Epoch 2/5] [Batch 86/166] [D loss: 1.322912, acc: 37%] [G loss: 1.604244]\n",
      "[Epoch 2/5] [Batch 87/166] [D loss: 1.383833, acc: 40%] [G loss: 1.289204]\n",
      "[Epoch 2/5] [Batch 88/166] [D loss: 1.413249, acc: 34%] [G loss: 1.373672]\n",
      "[Epoch 2/5] [Batch 89/166] [D loss: 1.298847, acc: 43%] [G loss: 1.483494]\n",
      "[Epoch 2/5] [Batch 90/166] [D loss: 1.145050, acc: 40%] [G loss: 1.192933]\n",
      "[Epoch 2/5] [Batch 91/166] [D loss: 1.145021, acc: 43%] [G loss: 1.233007]\n",
      "[Epoch 2/5] [Batch 92/166] [D loss: 1.484883, acc: 37%] [G loss: 1.436023]\n",
      "[Epoch 2/5] [Batch 93/166] [D loss: 1.253664, acc: 34%] [G loss: 1.583218]\n",
      "[Epoch 2/5] [Batch 94/166] [D loss: 1.177329, acc: 40%] [G loss: 1.283274]\n",
      "[Epoch 2/5] [Batch 95/166] [D loss: 1.267808, acc: 46%] [G loss: 1.693087]\n",
      "[Epoch 2/5] [Batch 96/166] [D loss: 1.243438, acc: 43%] [G loss: 1.468084]\n",
      "[Epoch 2/5] [Batch 97/166] [D loss: 1.261037, acc: 43%] [G loss: 1.662648]\n",
      "[Epoch 2/5] [Batch 98/166] [D loss: 1.505393, acc: 28%] [G loss: 1.814711]\n",
      "[Epoch 2/5] [Batch 99/166] [D loss: 1.419145, acc: 37%] [G loss: 1.498746]\n",
      "[Epoch 2/5] [Batch 100/166] [D loss: 1.274091, acc: 34%] [G loss: 1.271061]\n",
      "[Epoch 2/5] [Batch 101/166] [D loss: 1.468635, acc: 21%] [G loss: 1.381603]\n",
      "[Epoch 2/5] [Batch 102/166] [D loss: 1.312487, acc: 37%] [G loss: 1.343791]\n",
      "[Epoch 2/5] [Batch 103/166] [D loss: 1.324250, acc: 43%] [G loss: 0.988742]\n",
      "[Epoch 2/5] [Batch 104/166] [D loss: 1.141979, acc: 53%] [G loss: 1.022593]\n",
      "[Epoch 2/5] [Batch 105/166] [D loss: 1.160271, acc: 56%] [G loss: 1.193332]\n",
      "[Epoch 2/5] [Batch 106/166] [D loss: 1.433159, acc: 50%] [G loss: 1.264011]\n",
      "[Epoch 2/5] [Batch 107/166] [D loss: 1.323724, acc: 31%] [G loss: 1.805426]\n",
      "[Epoch 2/5] [Batch 108/166] [D loss: 1.382301, acc: 37%] [G loss: 1.650700]\n",
      "[Epoch 2/5] [Batch 109/166] [D loss: 1.436038, acc: 43%] [G loss: 1.189633]\n",
      "[Epoch 2/5] [Batch 110/166] [D loss: 1.365072, acc: 34%] [G loss: 1.284187]\n",
      "[Epoch 2/5] [Batch 111/166] [D loss: 1.667565, acc: 43%] [G loss: 1.411277]\n",
      "[Epoch 2/5] [Batch 112/166] [D loss: 1.406249, acc: 40%] [G loss: 1.522685]\n",
      "[Epoch 2/5] [Batch 113/166] [D loss: 1.558740, acc: 21%] [G loss: 1.745869]\n",
      "[Epoch 2/5] [Batch 114/166] [D loss: 1.300005, acc: 56%] [G loss: 1.293398]\n",
      "[Epoch 2/5] [Batch 115/166] [D loss: 1.354020, acc: 37%] [G loss: 1.135164]\n",
      "[Epoch 2/5] [Batch 116/166] [D loss: 1.369312, acc: 40%] [G loss: 1.103201]\n",
      "[Epoch 2/5] [Batch 117/166] [D loss: 1.343314, acc: 50%] [G loss: 1.351285]\n",
      "[Epoch 2/5] [Batch 118/166] [D loss: 1.306059, acc: 43%] [G loss: 1.454313]\n",
      "[Epoch 2/5] [Batch 119/166] [D loss: 1.342432, acc: 31%] [G loss: 1.291096]\n",
      "[Epoch 2/5] [Batch 120/166] [D loss: 1.340530, acc: 43%] [G loss: 1.535594]\n",
      "[Epoch 2/5] [Batch 121/166] [D loss: 1.469625, acc: 43%] [G loss: 1.551962]\n",
      "[Epoch 2/5] [Batch 122/166] [D loss: 1.373823, acc: 53%] [G loss: 1.665183]\n",
      "[Epoch 2/5] [Batch 123/166] [D loss: 1.403024, acc: 46%] [G loss: 1.411412]\n",
      "[Epoch 2/5] [Batch 124/166] [D loss: 1.307983, acc: 50%] [G loss: 1.222337]\n",
      "[Epoch 2/5] [Batch 125/166] [D loss: 1.302702, acc: 46%] [G loss: 1.101507]\n",
      "[Epoch 2/5] [Batch 126/166] [D loss: 1.214231, acc: 43%] [G loss: 1.384257]\n",
      "[Epoch 2/5] [Batch 127/166] [D loss: 1.490714, acc: 28%] [G loss: 1.452402]\n",
      "[Epoch 2/5] [Batch 128/166] [D loss: 1.250629, acc: 62%] [G loss: 1.319872]\n",
      "[Epoch 2/5] [Batch 129/166] [D loss: 1.456900, acc: 37%] [G loss: 1.586965]\n",
      "[Epoch 2/5] [Batch 130/166] [D loss: 1.451240, acc: 56%] [G loss: 1.125339]\n",
      "[Epoch 2/5] [Batch 131/166] [D loss: 1.357999, acc: 43%] [G loss: 1.269313]\n",
      "[Epoch 2/5] [Batch 132/166] [D loss: 1.480687, acc: 46%] [G loss: 1.158576]\n",
      "[Epoch 2/5] [Batch 133/166] [D loss: 1.250953, acc: 31%] [G loss: 1.514883]\n",
      "[Epoch 2/5] [Batch 134/166] [D loss: 1.472764, acc: 37%] [G loss: 1.944930]\n",
      "[Epoch 2/5] [Batch 135/166] [D loss: 1.146954, acc: 18%] [G loss: 1.662345]\n",
      "[Epoch 2/5] [Batch 136/166] [D loss: 1.198020, acc: 37%] [G loss: 1.855210]\n",
      "[Epoch 2/5] [Batch 137/166] [D loss: 1.343292, acc: 46%] [G loss: 1.367422]\n",
      "[Epoch 2/5] [Batch 138/166] [D loss: 1.149912, acc: 50%] [G loss: 1.679443]\n",
      "[Epoch 2/5] [Batch 139/166] [D loss: 1.395117, acc: 40%] [G loss: 1.609195]\n",
      "[Epoch 2/5] [Batch 140/166] [D loss: 1.324564, acc: 46%] [G loss: 1.268097]\n",
      "[Epoch 2/5] [Batch 141/166] [D loss: 1.248529, acc: 50%] [G loss: 1.401340]\n",
      "[Epoch 2/5] [Batch 142/166] [D loss: 1.172575, acc: 50%] [G loss: 1.445595]\n",
      "[Epoch 2/5] [Batch 143/166] [D loss: 1.461610, acc: 43%] [G loss: 1.324123]\n",
      "[Epoch 2/5] [Batch 144/166] [D loss: 1.353844, acc: 37%] [G loss: 1.412431]\n",
      "[Epoch 2/5] [Batch 145/166] [D loss: 1.316886, acc: 50%] [G loss: 1.262596]\n",
      "[Epoch 2/5] [Batch 146/166] [D loss: 1.277280, acc: 37%] [G loss: 1.766353]\n",
      "[Epoch 2/5] [Batch 147/166] [D loss: 1.288631, acc: 34%] [G loss: 1.462738]\n",
      "[Epoch 2/5] [Batch 148/166] [D loss: 1.413035, acc: 43%] [G loss: 1.475223]\n",
      "[Epoch 2/5] [Batch 149/166] [D loss: 1.346011, acc: 40%] [G loss: 1.528897]\n",
      "[Epoch 2/5] [Batch 150/166] [D loss: 1.255735, acc: 46%] [G loss: 1.265047]\n",
      "[Epoch 2/5] [Batch 151/166] [D loss: 1.258434, acc: 28%] [G loss: 1.225538]\n",
      "[Epoch 2/5] [Batch 152/166] [D loss: 1.183267, acc: 34%] [G loss: 1.388264]\n",
      "[Epoch 2/5] [Batch 153/166] [D loss: 1.126609, acc: 59%] [G loss: 2.051224]\n",
      "[Epoch 2/5] [Batch 154/166] [D loss: 1.418131, acc: 37%] [G loss: 1.291021]\n",
      "[Epoch 2/5] [Batch 155/166] [D loss: 1.371072, acc: 40%] [G loss: 1.506472]\n",
      "[Epoch 2/5] [Batch 156/166] [D loss: 1.176096, acc: 43%] [G loss: 1.516029]\n",
      "[Epoch 2/5] [Batch 157/166] [D loss: 1.130585, acc: 43%] [G loss: 1.213606]\n",
      "[Epoch 2/5] [Batch 158/166] [D loss: 1.317555, acc: 31%] [G loss: 1.206394]\n",
      "[Epoch 2/5] [Batch 159/166] [D loss: 1.138667, acc: 59%] [G loss: 1.314971]\n",
      "[Epoch 2/5] [Batch 160/166] [D loss: 1.486696, acc: 34%] [G loss: 1.325519]\n",
      "[Epoch 2/5] [Batch 161/166] [D loss: 1.253415, acc: 53%] [G loss: 1.267116]\n",
      "[Epoch 2/5] [Batch 162/166] [D loss: 1.221382, acc: 40%] [G loss: 1.605806]\n",
      "[Epoch 2/5] [Batch 163/166] [D loss: 1.382292, acc: 46%] [G loss: 1.289717]\n",
      "[Epoch 2/5] [Batch 164/166] [D loss: 1.236984, acc: 40%] [G loss: 1.457104]\n",
      "[Epoch 3/5] [Batch 0/166] [D loss: 1.314763, acc: 56%] [G loss: 1.776273]\n",
      "[Epoch 3/5] [Batch 1/166] [D loss: 1.144175, acc: 46%] [G loss: 1.327599]\n",
      "[Epoch 3/5] [Batch 2/166] [D loss: 1.226067, acc: 50%] [G loss: 1.358497]\n",
      "[Epoch 3/5] [Batch 3/166] [D loss: 1.190852, acc: 53%] [G loss: 1.517848]\n",
      "[Epoch 3/5] [Batch 4/166] [D loss: 0.993230, acc: 53%] [G loss: 1.353182]\n",
      "[Epoch 3/5] [Batch 5/166] [D loss: 1.264419, acc: 43%] [G loss: 1.737945]\n",
      "[Epoch 3/5] [Batch 6/166] [D loss: 1.236979, acc: 40%] [G loss: 1.359858]\n",
      "[Epoch 3/5] [Batch 7/166] [D loss: 1.407166, acc: 53%] [G loss: 1.376740]\n",
      "[Epoch 3/5] [Batch 8/166] [D loss: 1.381608, acc: 40%] [G loss: 1.579028]\n",
      "[Epoch 3/5] [Batch 9/166] [D loss: 1.257338, acc: 37%] [G loss: 1.475333]\n",
      "[Epoch 3/5] [Batch 10/166] [D loss: 1.169181, acc: 46%] [G loss: 1.232150]\n",
      "[Epoch 3/5] [Batch 11/166] [D loss: 1.251857, acc: 25%] [G loss: 1.294595]\n",
      "[Epoch 3/5] [Batch 12/166] [D loss: 1.207085, acc: 40%] [G loss: 1.545594]\n",
      "[Epoch 3/5] [Batch 13/166] [D loss: 1.312643, acc: 28%] [G loss: 1.595948]\n",
      "[Epoch 3/5] [Batch 14/166] [D loss: 1.425509, acc: 37%] [G loss: 1.414152]\n",
      "[Epoch 3/5] [Batch 15/166] [D loss: 1.559394, acc: 40%] [G loss: 1.420289]\n",
      "[Epoch 3/5] [Batch 16/166] [D loss: 1.359132, acc: 50%] [G loss: 1.737107]\n",
      "[Epoch 3/5] [Batch 17/166] [D loss: 1.285858, acc: 31%] [G loss: 1.751606]\n",
      "[Epoch 3/5] [Batch 18/166] [D loss: 1.313449, acc: 46%] [G loss: 1.496226]\n",
      "[Epoch 3/5] [Batch 19/166] [D loss: 1.245603, acc: 62%] [G loss: 1.484720]\n",
      "[Epoch 3/5] [Batch 20/166] [D loss: 1.143007, acc: 50%] [G loss: 1.119103]\n",
      "[Epoch 3/5] [Batch 21/166] [D loss: 1.383265, acc: 43%] [G loss: 1.311997]\n",
      "[Epoch 3/5] [Batch 22/166] [D loss: 1.565842, acc: 25%] [G loss: 1.136113]\n",
      "[Epoch 3/5] [Batch 23/166] [D loss: 1.259492, acc: 50%] [G loss: 1.226925]\n",
      "[Epoch 3/5] [Batch 24/166] [D loss: 1.382430, acc: 37%] [G loss: 1.295703]\n",
      "[Epoch 3/5] [Batch 25/166] [D loss: 1.328315, acc: 34%] [G loss: 1.161355]\n",
      "[Epoch 3/5] [Batch 26/166] [D loss: 1.222775, acc: 50%] [G loss: 1.613259]\n",
      "[Epoch 3/5] [Batch 27/166] [D loss: 1.320002, acc: 50%] [G loss: 1.300066]\n",
      "[Epoch 3/5] [Batch 28/166] [D loss: 1.552271, acc: 46%] [G loss: 1.427405]\n",
      "[Epoch 3/5] [Batch 29/166] [D loss: 1.443066, acc: 25%] [G loss: 1.559967]\n",
      "[Epoch 3/5] [Batch 30/166] [D loss: 1.408771, acc: 43%] [G loss: 1.137685]\n",
      "[Epoch 3/5] [Batch 31/166] [D loss: 1.277565, acc: 40%] [G loss: 1.276177]\n",
      "[Epoch 3/5] [Batch 32/166] [D loss: 1.320632, acc: 37%] [G loss: 1.232341]\n",
      "[Epoch 3/5] [Batch 33/166] [D loss: 1.157308, acc: 43%] [G loss: 1.369943]\n",
      "[Epoch 3/5] [Batch 34/166] [D loss: 1.310623, acc: 40%] [G loss: 1.259230]\n",
      "[Epoch 3/5] [Batch 35/166] [D loss: 1.341992, acc: 53%] [G loss: 1.539447]\n",
      "[Epoch 3/5] [Batch 36/166] [D loss: 1.192073, acc: 31%] [G loss: 1.349833]\n",
      "[Epoch 3/5] [Batch 37/166] [D loss: 1.481687, acc: 28%] [G loss: 1.231505]\n",
      "[Epoch 3/5] [Batch 38/166] [D loss: 1.550988, acc: 46%] [G loss: 1.172190]\n",
      "[Epoch 3/5] [Batch 39/166] [D loss: 1.093923, acc: 31%] [G loss: 1.483816]\n",
      "[Epoch 3/5] [Batch 40/166] [D loss: 1.478457, acc: 34%] [G loss: 1.161588]\n",
      "[Epoch 3/5] [Batch 41/166] [D loss: 1.242471, acc: 53%] [G loss: 1.273776]\n",
      "[Epoch 3/5] [Batch 42/166] [D loss: 1.605070, acc: 18%] [G loss: 1.262422]\n",
      "[Epoch 3/5] [Batch 43/166] [D loss: 1.361363, acc: 40%] [G loss: 1.510870]\n",
      "[Epoch 3/5] [Batch 44/166] [D loss: 1.257670, acc: 43%] [G loss: 1.635689]\n",
      "[Epoch 3/5] [Batch 45/166] [D loss: 1.209969, acc: 43%] [G loss: 1.821319]\n",
      "[Epoch 3/5] [Batch 46/166] [D loss: 1.447875, acc: 43%] [G loss: 1.618700]\n",
      "[Epoch 3/5] [Batch 47/166] [D loss: 1.378536, acc: 40%] [G loss: 1.394260]\n",
      "[Epoch 3/5] [Batch 48/166] [D loss: 1.174575, acc: 43%] [G loss: 1.500583]\n",
      "[Epoch 3/5] [Batch 49/166] [D loss: 1.463651, acc: 28%] [G loss: 1.601791]\n",
      "[Epoch 3/5] [Batch 50/166] [D loss: 1.441522, acc: 34%] [G loss: 1.117595]\n",
      "[Epoch 3/5] [Batch 51/166] [D loss: 1.346234, acc: 37%] [G loss: 1.254998]\n",
      "[Epoch 3/5] [Batch 52/166] [D loss: 1.098208, acc: 53%] [G loss: 1.286631]\n",
      "[Epoch 3/5] [Batch 53/166] [D loss: 1.403626, acc: 34%] [G loss: 1.491976]\n",
      "[Epoch 3/5] [Batch 54/166] [D loss: 1.177079, acc: 40%] [G loss: 1.268204]\n",
      "[Epoch 3/5] [Batch 55/166] [D loss: 1.390080, acc: 46%] [G loss: 1.063093]\n",
      "[Epoch 3/5] [Batch 56/166] [D loss: 1.232451, acc: 40%] [G loss: 1.349724]\n",
      "[Epoch 3/5] [Batch 57/166] [D loss: 1.567917, acc: 25%] [G loss: 1.414281]\n",
      "[Epoch 3/5] [Batch 58/166] [D loss: 1.173292, acc: 46%] [G loss: 1.226300]\n",
      "[Epoch 3/5] [Batch 59/166] [D loss: 1.361899, acc: 43%] [G loss: 1.418455]\n",
      "[Epoch 3/5] [Batch 60/166] [D loss: 1.217409, acc: 21%] [G loss: 1.701235]\n",
      "[Epoch 3/5] [Batch 61/166] [D loss: 1.275930, acc: 37%] [G loss: 1.292931]\n",
      "[Epoch 3/5] [Batch 62/166] [D loss: 1.227823, acc: 31%] [G loss: 1.377092]\n",
      "[Epoch 3/5] [Batch 63/166] [D loss: 1.186928, acc: 34%] [G loss: 1.643269]\n",
      "[Epoch 3/5] [Batch 64/166] [D loss: 1.300717, acc: 46%] [G loss: 1.460230]\n",
      "[Epoch 3/5] [Batch 65/166] [D loss: 1.441144, acc: 53%] [G loss: 1.485755]\n",
      "[Epoch 3/5] [Batch 66/166] [D loss: 1.169894, acc: 31%] [G loss: 1.531847]\n",
      "[Epoch 3/5] [Batch 67/166] [D loss: 1.267623, acc: 43%] [G loss: 1.488895]\n",
      "[Epoch 3/5] [Batch 68/166] [D loss: 1.280251, acc: 37%] [G loss: 1.447882]\n",
      "[Epoch 3/5] [Batch 69/166] [D loss: 1.300253, acc: 34%] [G loss: 1.850626]\n",
      "[Epoch 3/5] [Batch 70/166] [D loss: 1.221766, acc: 37%] [G loss: 1.258971]\n",
      "[Epoch 3/5] [Batch 71/166] [D loss: 1.101547, acc: 43%] [G loss: 1.208859]\n",
      "[Epoch 3/5] [Batch 72/166] [D loss: 1.466463, acc: 40%] [G loss: 1.341979]\n",
      "[Epoch 3/5] [Batch 73/166] [D loss: 1.214121, acc: 46%] [G loss: 1.369880]\n",
      "[Epoch 3/5] [Batch 74/166] [D loss: 1.245220, acc: 46%] [G loss: 1.400529]\n",
      "[Epoch 3/5] [Batch 75/166] [D loss: 1.190122, acc: 43%] [G loss: 1.391364]\n",
      "[Epoch 3/5] [Batch 76/166] [D loss: 1.384627, acc: 46%] [G loss: 1.300702]\n",
      "[Epoch 3/5] [Batch 77/166] [D loss: 1.201045, acc: 46%] [G loss: 1.476865]\n",
      "[Epoch 3/5] [Batch 78/166] [D loss: 1.182113, acc: 40%] [G loss: 1.870148]\n",
      "[Epoch 3/5] [Batch 79/166] [D loss: 1.062181, acc: 46%] [G loss: 1.410328]\n",
      "[Epoch 3/5] [Batch 80/166] [D loss: 1.273992, acc: 28%] [G loss: 1.640094]\n",
      "[Epoch 3/5] [Batch 81/166] [D loss: 1.193274, acc: 46%] [G loss: 1.339553]\n",
      "[Epoch 3/5] [Batch 82/166] [D loss: 1.332227, acc: 21%] [G loss: 1.363869]\n",
      "[Epoch 3/5] [Batch 83/166] [D loss: 1.196976, acc: 31%] [G loss: 1.374162]\n",
      "[Epoch 3/5] [Batch 84/166] [D loss: 1.502013, acc: 28%] [G loss: 1.564323]\n",
      "[Epoch 3/5] [Batch 85/166] [D loss: 1.190046, acc: 37%] [G loss: 1.704647]\n",
      "[Epoch 3/5] [Batch 86/166] [D loss: 1.299926, acc: 34%] [G loss: 1.381212]\n",
      "[Epoch 3/5] [Batch 87/166] [D loss: 1.286473, acc: 37%] [G loss: 1.330547]\n",
      "[Epoch 3/5] [Batch 88/166] [D loss: 1.350106, acc: 34%] [G loss: 1.405745]\n",
      "[Epoch 3/5] [Batch 89/166] [D loss: 1.310754, acc: 31%] [G loss: 1.597526]\n",
      "[Epoch 3/5] [Batch 90/166] [D loss: 1.315663, acc: 21%] [G loss: 1.233183]\n",
      "[Epoch 3/5] [Batch 91/166] [D loss: 1.321316, acc: 31%] [G loss: 1.510102]\n",
      "[Epoch 3/5] [Batch 92/166] [D loss: 1.136985, acc: 53%] [G loss: 1.826495]\n",
      "[Epoch 3/5] [Batch 93/166] [D loss: 1.668297, acc: 37%] [G loss: 1.282604]\n",
      "[Epoch 3/5] [Batch 94/166] [D loss: 1.231720, acc: 37%] [G loss: 1.223764]\n",
      "[Epoch 3/5] [Batch 95/166] [D loss: 1.443980, acc: 50%] [G loss: 1.821996]\n",
      "[Epoch 3/5] [Batch 96/166] [D loss: 1.278978, acc: 31%] [G loss: 1.370101]\n",
      "[Epoch 3/5] [Batch 97/166] [D loss: 1.261968, acc: 31%] [G loss: 1.293318]\n",
      "[Epoch 3/5] [Batch 98/166] [D loss: 1.258691, acc: 34%] [G loss: 1.304508]\n",
      "[Epoch 3/5] [Batch 99/166] [D loss: 1.358157, acc: 18%] [G loss: 1.655437]\n",
      "[Epoch 3/5] [Batch 100/166] [D loss: 1.250554, acc: 43%] [G loss: 1.251741]\n",
      "[Epoch 3/5] [Batch 101/166] [D loss: 1.143350, acc: 28%] [G loss: 1.263617]\n",
      "[Epoch 3/5] [Batch 102/166] [D loss: 1.118471, acc: 56%] [G loss: 1.457497]\n",
      "[Epoch 3/5] [Batch 103/166] [D loss: 1.364298, acc: 21%] [G loss: 1.271187]\n",
      "[Epoch 3/5] [Batch 104/166] [D loss: 1.225332, acc: 50%] [G loss: 1.422163]\n",
      "[Epoch 3/5] [Batch 105/166] [D loss: 1.283735, acc: 31%] [G loss: 1.635897]\n",
      "[Epoch 3/5] [Batch 106/166] [D loss: 1.238162, acc: 50%] [G loss: 1.462847]\n",
      "[Epoch 3/5] [Batch 107/166] [D loss: 1.223046, acc: 46%] [G loss: 1.670434]\n",
      "[Epoch 3/5] [Batch 108/166] [D loss: 1.196810, acc: 28%] [G loss: 1.401818]\n",
      "[Epoch 3/5] [Batch 109/166] [D loss: 1.344911, acc: 37%] [G loss: 1.352391]\n",
      "[Epoch 3/5] [Batch 110/166] [D loss: 1.233479, acc: 31%] [G loss: 1.361242]\n",
      "[Epoch 3/5] [Batch 111/166] [D loss: 1.299658, acc: 34%] [G loss: 1.298898]\n",
      "[Epoch 3/5] [Batch 112/166] [D loss: 1.443470, acc: 43%] [G loss: 1.295551]\n",
      "[Epoch 3/5] [Batch 113/166] [D loss: 1.510445, acc: 37%] [G loss: 1.765287]\n",
      "[Epoch 3/5] [Batch 114/166] [D loss: 1.244494, acc: 46%] [G loss: 1.448644]\n",
      "[Epoch 3/5] [Batch 115/166] [D loss: 1.191126, acc: 50%] [G loss: 1.276345]\n",
      "[Epoch 3/5] [Batch 116/166] [D loss: 1.264542, acc: 43%] [G loss: 1.240747]\n",
      "[Epoch 3/5] [Batch 117/166] [D loss: 1.215692, acc: 46%] [G loss: 1.144249]\n",
      "[Epoch 3/5] [Batch 118/166] [D loss: 1.174349, acc: 43%] [G loss: 1.167323]\n",
      "[Epoch 3/5] [Batch 119/166] [D loss: 1.291701, acc: 37%] [G loss: 1.104274]\n",
      "[Epoch 3/5] [Batch 120/166] [D loss: 1.306489, acc: 43%] [G loss: 1.313303]\n",
      "[Epoch 3/5] [Batch 121/166] [D loss: 1.338655, acc: 46%] [G loss: 1.544883]\n",
      "[Epoch 3/5] [Batch 122/166] [D loss: 1.299409, acc: 37%] [G loss: 1.305342]\n",
      "[Epoch 3/5] [Batch 123/166] [D loss: 1.201795, acc: 37%] [G loss: 1.423797]\n",
      "[Epoch 3/5] [Batch 124/166] [D loss: 1.215986, acc: 40%] [G loss: 1.408623]\n",
      "[Epoch 3/5] [Batch 125/166] [D loss: 1.176428, acc: 43%] [G loss: 1.343654]\n",
      "[Epoch 3/5] [Batch 126/166] [D loss: 1.193785, acc: 40%] [G loss: 1.293737]\n",
      "[Epoch 3/5] [Batch 127/166] [D loss: 1.215299, acc: 40%] [G loss: 1.279186]\n",
      "[Epoch 3/5] [Batch 128/166] [D loss: 1.411398, acc: 37%] [G loss: 1.598423]\n",
      "[Epoch 3/5] [Batch 129/166] [D loss: 1.383818, acc: 40%] [G loss: 1.191432]\n",
      "[Epoch 3/5] [Batch 130/166] [D loss: 1.267344, acc: 56%] [G loss: 1.188740]\n",
      "[Epoch 3/5] [Batch 131/166] [D loss: 1.215639, acc: 43%] [G loss: 1.252407]\n",
      "[Epoch 3/5] [Batch 132/166] [D loss: 1.201331, acc: 46%] [G loss: 1.938917]\n",
      "[Epoch 3/5] [Batch 133/166] [D loss: 1.141253, acc: 40%] [G loss: 1.570068]\n",
      "[Epoch 3/5] [Batch 134/166] [D loss: 1.258406, acc: 37%] [G loss: 1.685318]\n",
      "[Epoch 3/5] [Batch 135/166] [D loss: 1.141682, acc: 31%] [G loss: 1.296204]\n",
      "[Epoch 3/5] [Batch 136/166] [D loss: 1.370062, acc: 43%] [G loss: 1.377767]\n",
      "[Epoch 3/5] [Batch 137/166] [D loss: 1.608748, acc: 46%] [G loss: 1.487519]\n",
      "[Epoch 3/5] [Batch 138/166] [D loss: 1.158142, acc: 53%] [G loss: 1.558133]\n",
      "[Epoch 3/5] [Batch 139/166] [D loss: 1.273345, acc: 34%] [G loss: 1.325866]\n",
      "[Epoch 3/5] [Batch 140/166] [D loss: 1.308068, acc: 40%] [G loss: 1.045167]\n",
      "[Epoch 3/5] [Batch 141/166] [D loss: 1.169224, acc: 50%] [G loss: 1.155500]\n",
      "[Epoch 3/5] [Batch 142/166] [D loss: 1.270156, acc: 34%] [G loss: 1.473564]\n",
      "[Epoch 3/5] [Batch 143/166] [D loss: 1.343385, acc: 37%] [G loss: 1.493627]\n",
      "[Epoch 3/5] [Batch 144/166] [D loss: 1.162747, acc: 34%] [G loss: 1.516754]\n",
      "[Epoch 3/5] [Batch 145/166] [D loss: 1.529592, acc: 28%] [G loss: 1.558010]\n",
      "[Epoch 3/5] [Batch 146/166] [D loss: 1.427420, acc: 37%] [G loss: 1.298920]\n",
      "[Epoch 3/5] [Batch 147/166] [D loss: 1.283878, acc: 40%] [G loss: 1.247890]\n",
      "[Epoch 3/5] [Batch 148/166] [D loss: 1.218085, acc: 31%] [G loss: 1.536166]\n",
      "[Epoch 3/5] [Batch 149/166] [D loss: 1.396660, acc: 53%] [G loss: 1.617752]\n",
      "[Epoch 3/5] [Batch 150/166] [D loss: 1.549854, acc: 40%] [G loss: 1.647728]\n",
      "[Epoch 3/5] [Batch 151/166] [D loss: 1.309754, acc: 37%] [G loss: 1.766928]\n",
      "[Epoch 3/5] [Batch 152/166] [D loss: 1.548649, acc: 37%] [G loss: 1.488810]\n",
      "[Epoch 3/5] [Batch 153/166] [D loss: 1.301311, acc: 43%] [G loss: 1.384093]\n",
      "[Epoch 3/5] [Batch 154/166] [D loss: 1.256516, acc: 34%] [G loss: 1.292653]\n",
      "[Epoch 3/5] [Batch 155/166] [D loss: 1.333196, acc: 43%] [G loss: 1.266169]\n",
      "[Epoch 3/5] [Batch 156/166] [D loss: 1.422225, acc: 34%] [G loss: 1.347600]\n",
      "[Epoch 3/5] [Batch 157/166] [D loss: 1.166789, acc: 50%] [G loss: 1.429243]\n",
      "[Epoch 3/5] [Batch 158/166] [D loss: 1.432173, acc: 43%] [G loss: 1.338923]\n",
      "[Epoch 3/5] [Batch 159/166] [D loss: 1.274004, acc: 43%] [G loss: 1.384214]\n",
      "[Epoch 3/5] [Batch 160/166] [D loss: 1.239153, acc: 46%] [G loss: 1.662408]\n",
      "[Epoch 3/5] [Batch 161/166] [D loss: 1.248736, acc: 43%] [G loss: 1.218665]\n",
      "[Epoch 3/5] [Batch 162/166] [D loss: 1.398553, acc: 50%] [G loss: 1.148307]\n",
      "[Epoch 3/5] [Batch 163/166] [D loss: 1.432559, acc: 40%] [G loss: 1.453451]\n",
      "[Epoch 3/5] [Batch 164/166] [D loss: 1.364931, acc: 43%] [G loss: 1.143450]\n",
      "[Epoch 4/5] [Batch 0/166] [D loss: 1.214298, acc: 50%] [G loss: 1.594319]\n",
      "[Epoch 4/5] [Batch 1/166] [D loss: 1.207109, acc: 40%] [G loss: 1.262713]\n",
      "[Epoch 4/5] [Batch 2/166] [D loss: 1.513783, acc: 43%] [G loss: 1.392356]\n",
      "[Epoch 4/5] [Batch 3/166] [D loss: 1.212569, acc: 59%] [G loss: 1.397238]\n",
      "[Epoch 4/5] [Batch 4/166] [D loss: 1.269682, acc: 53%] [G loss: 1.446232]\n",
      "[Epoch 4/5] [Batch 5/166] [D loss: 1.364042, acc: 50%] [G loss: 1.578943]\n",
      "[Epoch 4/5] [Batch 6/166] [D loss: 1.284884, acc: 43%] [G loss: 1.196523]\n",
      "[Epoch 4/5] [Batch 7/166] [D loss: 1.345524, acc: 56%] [G loss: 1.328606]\n",
      "[Epoch 4/5] [Batch 8/166] [D loss: 1.190261, acc: 31%] [G loss: 1.253479]\n",
      "[Epoch 4/5] [Batch 9/166] [D loss: 1.447187, acc: 34%] [G loss: 1.205644]\n",
      "[Epoch 4/5] [Batch 10/166] [D loss: 1.177690, acc: 46%] [G loss: 1.412220]\n",
      "[Epoch 4/5] [Batch 11/166] [D loss: 1.449993, acc: 43%] [G loss: 1.314039]\n",
      "[Epoch 4/5] [Batch 12/166] [D loss: 1.320318, acc: 46%] [G loss: 1.318849]\n",
      "[Epoch 4/5] [Batch 13/166] [D loss: 1.305302, acc: 43%] [G loss: 1.246677]\n",
      "[Epoch 4/5] [Batch 14/166] [D loss: 1.291053, acc: 34%] [G loss: 1.367681]\n",
      "[Epoch 4/5] [Batch 15/166] [D loss: 1.302637, acc: 37%] [G loss: 1.412573]\n",
      "[Epoch 4/5] [Batch 16/166] [D loss: 1.108633, acc: 34%] [G loss: 1.357134]\n",
      "[Epoch 4/5] [Batch 17/166] [D loss: 1.348289, acc: 40%] [G loss: 1.643484]\n",
      "[Epoch 4/5] [Batch 18/166] [D loss: 1.437480, acc: 53%] [G loss: 1.686648]\n",
      "[Epoch 4/5] [Batch 19/166] [D loss: 1.317940, acc: 50%] [G loss: 1.169029]\n",
      "[Epoch 4/5] [Batch 20/166] [D loss: 1.266769, acc: 37%] [G loss: 1.692595]\n",
      "[Epoch 4/5] [Batch 21/166] [D loss: 1.206283, acc: 53%] [G loss: 1.347207]\n",
      "[Epoch 4/5] [Batch 22/166] [D loss: 1.253373, acc: 40%] [G loss: 1.509896]\n",
      "[Epoch 4/5] [Batch 23/166] [D loss: 1.595214, acc: 37%] [G loss: 1.109284]\n",
      "[Epoch 4/5] [Batch 24/166] [D loss: 1.203569, acc: 40%] [G loss: 1.485948]\n",
      "[Epoch 4/5] [Batch 25/166] [D loss: 1.155699, acc: 53%] [G loss: 1.438287]\n",
      "[Epoch 4/5] [Batch 26/166] [D loss: 1.182365, acc: 37%] [G loss: 1.222049]\n",
      "[Epoch 4/5] [Batch 27/166] [D loss: 1.333568, acc: 46%] [G loss: 1.124653]\n",
      "[Epoch 4/5] [Batch 28/166] [D loss: 1.271351, acc: 50%] [G loss: 1.237119]\n",
      "[Epoch 4/5] [Batch 29/166] [D loss: 1.211877, acc: 31%] [G loss: 1.125855]\n",
      "[Epoch 4/5] [Batch 30/166] [D loss: 1.440573, acc: 43%] [G loss: 1.364692]\n",
      "[Epoch 4/5] [Batch 31/166] [D loss: 1.120453, acc: 50%] [G loss: 1.068127]\n",
      "[Epoch 4/5] [Batch 32/166] [D loss: 1.219910, acc: 53%] [G loss: 1.389188]\n",
      "[Epoch 4/5] [Batch 33/166] [D loss: 1.235736, acc: 31%] [G loss: 1.445043]\n",
      "[Epoch 4/5] [Batch 34/166] [D loss: 1.299660, acc: 34%] [G loss: 1.195721]\n",
      "[Epoch 4/5] [Batch 35/166] [D loss: 1.309448, acc: 37%] [G loss: 1.502472]\n",
      "[Epoch 4/5] [Batch 36/166] [D loss: 1.338968, acc: 28%] [G loss: 1.621633]\n",
      "[Epoch 4/5] [Batch 37/166] [D loss: 1.221138, acc: 37%] [G loss: 1.597524]\n",
      "[Epoch 4/5] [Batch 38/166] [D loss: 1.387229, acc: 28%] [G loss: 1.576416]\n",
      "[Epoch 4/5] [Batch 39/166] [D loss: 1.335424, acc: 40%] [G loss: 1.315644]\n",
      "[Epoch 4/5] [Batch 40/166] [D loss: 1.262148, acc: 46%] [G loss: 1.675155]\n",
      "[Epoch 4/5] [Batch 41/166] [D loss: 1.167161, acc: 50%] [G loss: 1.243824]\n",
      "[Epoch 4/5] [Batch 42/166] [D loss: 1.684186, acc: 31%] [G loss: 1.430295]\n",
      "[Epoch 4/5] [Batch 43/166] [D loss: 1.220017, acc: 40%] [G loss: 1.527397]\n",
      "[Epoch 4/5] [Batch 44/166] [D loss: 1.433568, acc: 28%] [G loss: 1.514065]\n",
      "[Epoch 4/5] [Batch 45/166] [D loss: 1.267997, acc: 34%] [G loss: 1.728420]\n",
      "[Epoch 4/5] [Batch 46/166] [D loss: 1.450978, acc: 46%] [G loss: 1.267044]\n",
      "[Epoch 4/5] [Batch 47/166] [D loss: 1.369885, acc: 34%] [G loss: 1.587890]\n",
      "[Epoch 4/5] [Batch 48/166] [D loss: 1.152912, acc: 46%] [G loss: 1.108935]\n",
      "[Epoch 4/5] [Batch 49/166] [D loss: 1.400916, acc: 34%] [G loss: 1.562433]\n",
      "[Epoch 4/5] [Batch 50/166] [D loss: 1.529675, acc: 28%] [G loss: 1.318216]\n",
      "[Epoch 4/5] [Batch 51/166] [D loss: 1.479792, acc: 37%] [G loss: 1.631351]\n",
      "[Epoch 4/5] [Batch 52/166] [D loss: 1.323257, acc: 37%] [G loss: 1.626208]\n",
      "[Epoch 4/5] [Batch 53/166] [D loss: 1.308325, acc: 34%] [G loss: 1.424473]\n",
      "[Epoch 4/5] [Batch 54/166] [D loss: 1.350477, acc: 25%] [G loss: 1.337402]\n",
      "[Epoch 4/5] [Batch 55/166] [D loss: 1.426708, acc: 37%] [G loss: 1.287654]\n",
      "[Epoch 4/5] [Batch 56/166] [D loss: 1.138384, acc: 50%] [G loss: 1.569943]\n",
      "[Epoch 4/5] [Batch 57/166] [D loss: 1.240775, acc: 31%] [G loss: 1.590432]\n",
      "[Epoch 4/5] [Batch 58/166] [D loss: 1.356755, acc: 40%] [G loss: 1.230893]\n",
      "[Epoch 4/5] [Batch 59/166] [D loss: 1.317233, acc: 37%] [G loss: 1.277577]\n",
      "[Epoch 4/5] [Batch 60/166] [D loss: 1.259784, acc: 31%] [G loss: 1.533639]\n",
      "[Epoch 4/5] [Batch 61/166] [D loss: 1.110216, acc: 53%] [G loss: 1.615027]\n",
      "[Epoch 4/5] [Batch 62/166] [D loss: 1.338454, acc: 31%] [G loss: 1.307048]\n",
      "[Epoch 4/5] [Batch 63/166] [D loss: 1.186405, acc: 43%] [G loss: 1.056887]\n",
      "[Epoch 4/5] [Batch 64/166] [D loss: 1.298891, acc: 34%] [G loss: 1.314619]\n",
      "[Epoch 4/5] [Batch 65/166] [D loss: 1.388716, acc: 46%] [G loss: 1.152995]\n",
      "[Epoch 4/5] [Batch 66/166] [D loss: 1.302039, acc: 34%] [G loss: 1.289943]\n",
      "[Epoch 4/5] [Batch 67/166] [D loss: 1.285526, acc: 43%] [G loss: 1.212583]\n",
      "[Epoch 4/5] [Batch 68/166] [D loss: 1.279359, acc: 40%] [G loss: 1.394736]\n",
      "[Epoch 4/5] [Batch 69/166] [D loss: 1.414150, acc: 34%] [G loss: 1.068184]\n",
      "[Epoch 4/5] [Batch 70/166] [D loss: 1.363545, acc: 43%] [G loss: 1.315093]\n",
      "[Epoch 4/5] [Batch 71/166] [D loss: 1.406770, acc: 46%] [G loss: 1.154601]\n",
      "[Epoch 4/5] [Batch 72/166] [D loss: 1.426773, acc: 50%] [G loss: 1.311582]\n",
      "[Epoch 4/5] [Batch 73/166] [D loss: 1.217758, acc: 37%] [G loss: 1.263603]\n",
      "[Epoch 4/5] [Batch 74/166] [D loss: 1.281661, acc: 50%] [G loss: 1.626508]\n",
      "[Epoch 4/5] [Batch 75/166] [D loss: 1.192947, acc: 34%] [G loss: 1.453836]\n",
      "[Epoch 4/5] [Batch 76/166] [D loss: 1.308356, acc: 40%] [G loss: 1.105467]\n",
      "[Epoch 4/5] [Batch 77/166] [D loss: 1.258110, acc: 34%] [G loss: 1.496718]\n",
      "[Epoch 4/5] [Batch 78/166] [D loss: 1.197960, acc: 46%] [G loss: 1.318631]\n",
      "[Epoch 4/5] [Batch 79/166] [D loss: 1.377701, acc: 50%] [G loss: 1.215990]\n",
      "[Epoch 4/5] [Batch 80/166] [D loss: 1.335722, acc: 43%] [G loss: 1.017852]\n",
      "[Epoch 4/5] [Batch 81/166] [D loss: 1.445931, acc: 21%] [G loss: 1.554698]\n",
      "[Epoch 4/5] [Batch 82/166] [D loss: 1.270718, acc: 28%] [G loss: 1.504559]\n",
      "[Epoch 4/5] [Batch 83/166] [D loss: 1.034811, acc: 43%] [G loss: 1.329449]\n",
      "[Epoch 4/5] [Batch 84/166] [D loss: 1.342162, acc: 50%] [G loss: 1.376806]\n",
      "[Epoch 4/5] [Batch 85/166] [D loss: 1.198734, acc: 28%] [G loss: 1.449070]\n",
      "[Epoch 4/5] [Batch 86/166] [D loss: 1.213519, acc: 28%] [G loss: 1.284567]\n",
      "[Epoch 4/5] [Batch 87/166] [D loss: 1.116184, acc: 53%] [G loss: 1.211417]\n",
      "[Epoch 4/5] [Batch 88/166] [D loss: 1.123798, acc: 50%] [G loss: 1.353689]\n",
      "[Epoch 4/5] [Batch 89/166] [D loss: 1.117936, acc: 37%] [G loss: 1.333655]\n",
      "[Epoch 4/5] [Batch 90/166] [D loss: 1.072321, acc: 46%] [G loss: 1.517609]\n",
      "[Epoch 4/5] [Batch 91/166] [D loss: 1.451048, acc: 40%] [G loss: 1.442518]\n",
      "[Epoch 4/5] [Batch 92/166] [D loss: 1.298278, acc: 43%] [G loss: 1.581415]\n",
      "[Epoch 4/5] [Batch 93/166] [D loss: 1.233064, acc: 53%] [G loss: 1.519139]\n",
      "[Epoch 4/5] [Batch 94/166] [D loss: 1.228682, acc: 43%] [G loss: 1.231055]\n",
      "[Epoch 4/5] [Batch 95/166] [D loss: 1.160173, acc: 53%] [G loss: 1.171371]\n",
      "[Epoch 4/5] [Batch 96/166] [D loss: 1.330437, acc: 37%] [G loss: 1.369985]\n",
      "[Epoch 4/5] [Batch 97/166] [D loss: 1.361573, acc: 40%] [G loss: 1.334915]\n",
      "[Epoch 4/5] [Batch 98/166] [D loss: 1.241340, acc: 43%] [G loss: 1.468871]\n",
      "[Epoch 4/5] [Batch 99/166] [D loss: 1.437142, acc: 43%] [G loss: 1.400968]\n",
      "[Epoch 4/5] [Batch 100/166] [D loss: 1.431715, acc: 34%] [G loss: 1.135858]\n",
      "[Epoch 4/5] [Batch 101/166] [D loss: 1.289621, acc: 28%] [G loss: 1.534984]\n",
      "[Epoch 4/5] [Batch 102/166] [D loss: 1.332018, acc: 40%] [G loss: 1.661423]\n",
      "[Epoch 4/5] [Batch 103/166] [D loss: 1.383350, acc: 40%] [G loss: 1.161045]\n",
      "[Epoch 4/5] [Batch 104/166] [D loss: 1.253520, acc: 46%] [G loss: 1.392953]\n",
      "[Epoch 4/5] [Batch 105/166] [D loss: 1.275821, acc: 43%] [G loss: 1.265296]\n",
      "[Epoch 4/5] [Batch 106/166] [D loss: 1.446048, acc: 46%] [G loss: 1.211063]\n",
      "[Epoch 4/5] [Batch 107/166] [D loss: 1.359277, acc: 43%] [G loss: 1.343567]\n",
      "[Epoch 4/5] [Batch 108/166] [D loss: 1.135330, acc: 46%] [G loss: 1.562523]\n",
      "[Epoch 4/5] [Batch 109/166] [D loss: 1.397425, acc: 28%] [G loss: 1.436789]\n",
      "[Epoch 4/5] [Batch 110/166] [D loss: 1.381862, acc: 40%] [G loss: 1.066675]\n",
      "[Epoch 4/5] [Batch 111/166] [D loss: 1.559548, acc: 34%] [G loss: 1.169081]\n",
      "[Epoch 4/5] [Batch 112/166] [D loss: 1.040852, acc: 53%] [G loss: 1.504093]\n",
      "[Epoch 4/5] [Batch 113/166] [D loss: 1.147339, acc: 34%] [G loss: 1.288718]\n",
      "[Epoch 4/5] [Batch 114/166] [D loss: 1.364687, acc: 40%] [G loss: 1.386304]\n",
      "[Epoch 4/5] [Batch 115/166] [D loss: 1.274516, acc: 53%] [G loss: 1.441847]\n",
      "[Epoch 4/5] [Batch 116/166] [D loss: 1.403769, acc: 37%] [G loss: 1.632663]\n",
      "[Epoch 4/5] [Batch 117/166] [D loss: 1.454373, acc: 40%] [G loss: 1.374806]\n",
      "[Epoch 4/5] [Batch 118/166] [D loss: 1.218703, acc: 40%] [G loss: 1.243142]\n",
      "[Epoch 4/5] [Batch 119/166] [D loss: 1.247851, acc: 37%] [G loss: 1.353403]\n",
      "[Epoch 4/5] [Batch 120/166] [D loss: 1.142311, acc: 37%] [G loss: 1.499287]\n",
      "[Epoch 4/5] [Batch 121/166] [D loss: 1.151008, acc: 53%] [G loss: 1.591347]\n",
      "[Epoch 4/5] [Batch 122/166] [D loss: 1.349820, acc: 31%] [G loss: 1.836837]\n",
      "[Epoch 4/5] [Batch 123/166] [D loss: 1.407934, acc: 43%] [G loss: 1.219105]\n",
      "[Epoch 4/5] [Batch 124/166] [D loss: 1.213553, acc: 53%] [G loss: 1.337395]\n",
      "[Epoch 4/5] [Batch 125/166] [D loss: 1.296974, acc: 15%] [G loss: 1.381260]\n",
      "[Epoch 4/5] [Batch 126/166] [D loss: 1.287727, acc: 43%] [G loss: 1.438161]\n",
      "[Epoch 4/5] [Batch 127/166] [D loss: 1.194423, acc: 34%] [G loss: 1.448655]\n",
      "[Epoch 4/5] [Batch 128/166] [D loss: 1.463024, acc: 43%] [G loss: 1.267446]\n",
      "[Epoch 4/5] [Batch 129/166] [D loss: 1.344819, acc: 40%] [G loss: 1.527440]\n",
      "[Epoch 4/5] [Batch 130/166] [D loss: 1.180827, acc: 40%] [G loss: 1.372253]\n",
      "[Epoch 4/5] [Batch 131/166] [D loss: 1.231590, acc: 37%] [G loss: 1.381642]\n",
      "[Epoch 4/5] [Batch 132/166] [D loss: 1.387875, acc: 53%] [G loss: 1.272056]\n",
      "[Epoch 4/5] [Batch 133/166] [D loss: 1.166733, acc: 40%] [G loss: 1.629557]\n",
      "[Epoch 4/5] [Batch 134/166] [D loss: 1.275790, acc: 40%] [G loss: 1.486204]\n",
      "[Epoch 4/5] [Batch 135/166] [D loss: 1.447934, acc: 37%] [G loss: 1.513485]\n",
      "[Epoch 4/5] [Batch 136/166] [D loss: 1.199728, acc: 53%] [G loss: 1.421373]\n",
      "[Epoch 4/5] [Batch 137/166] [D loss: 1.145708, acc: 37%] [G loss: 1.406753]\n",
      "[Epoch 4/5] [Batch 138/166] [D loss: 1.235233, acc: 43%] [G loss: 1.378671]\n",
      "[Epoch 4/5] [Batch 139/166] [D loss: 1.279910, acc: 40%] [G loss: 1.156427]\n",
      "[Epoch 4/5] [Batch 140/166] [D loss: 1.429530, acc: 46%] [G loss: 1.204195]\n",
      "[Epoch 4/5] [Batch 141/166] [D loss: 1.242676, acc: 53%] [G loss: 1.176612]\n",
      "[Epoch 4/5] [Batch 142/166] [D loss: 1.470324, acc: 28%] [G loss: 1.476398]\n",
      "[Epoch 4/5] [Batch 143/166] [D loss: 1.280186, acc: 43%] [G loss: 1.269966]\n",
      "[Epoch 4/5] [Batch 144/166] [D loss: 1.227833, acc: 37%] [G loss: 1.269264]\n",
      "[Epoch 4/5] [Batch 145/166] [D loss: 1.259305, acc: 37%] [G loss: 1.579829]\n",
      "[Epoch 4/5] [Batch 146/166] [D loss: 1.289572, acc: 43%] [G loss: 1.098028]\n",
      "[Epoch 4/5] [Batch 147/166] [D loss: 1.293728, acc: 37%] [G loss: 1.439530]\n",
      "[Epoch 4/5] [Batch 148/166] [D loss: 1.440158, acc: 34%] [G loss: 1.416540]\n",
      "[Epoch 4/5] [Batch 149/166] [D loss: 1.454166, acc: 37%] [G loss: 1.428875]\n",
      "[Epoch 4/5] [Batch 150/166] [D loss: 1.264627, acc: 43%] [G loss: 1.292328]\n",
      "[Epoch 4/5] [Batch 151/166] [D loss: 1.227236, acc: 28%] [G loss: 1.609724]\n",
      "[Epoch 4/5] [Batch 152/166] [D loss: 1.170619, acc: 37%] [G loss: 1.122677]\n",
      "[Epoch 4/5] [Batch 153/166] [D loss: 1.415653, acc: 34%] [G loss: 1.556079]\n",
      "[Epoch 4/5] [Batch 154/166] [D loss: 1.055551, acc: 56%] [G loss: 1.526414]\n",
      "[Epoch 4/5] [Batch 155/166] [D loss: 1.231564, acc: 37%] [G loss: 1.221157]\n",
      "[Epoch 4/5] [Batch 156/166] [D loss: 1.421871, acc: 40%] [G loss: 1.305712]\n",
      "[Epoch 4/5] [Batch 157/166] [D loss: 1.130121, acc: 37%] [G loss: 1.279615]\n",
      "[Epoch 4/5] [Batch 158/166] [D loss: 1.246562, acc: 25%] [G loss: 1.339204]\n",
      "[Epoch 4/5] [Batch 159/166] [D loss: 1.501140, acc: 43%] [G loss: 1.107791]\n",
      "[Epoch 4/5] [Batch 160/166] [D loss: 1.279585, acc: 40%] [G loss: 1.483455]\n",
      "[Epoch 4/5] [Batch 161/166] [D loss: 1.213534, acc: 37%] [G loss: 1.530847]\n",
      "[Epoch 4/5] [Batch 162/166] [D loss: 1.272932, acc: 59%] [G loss: 1.452733]\n",
      "[Epoch 4/5] [Batch 163/166] [D loss: 1.545745, acc: 31%] [G loss: 1.541058]\n",
      "[Epoch 4/5] [Batch 164/166] [D loss: 1.346248, acc: 37%] [G loss: 1.349454]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "device='cuda'\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        if i<165:\n",
    "        \n",
    "            batch_size = 16\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            # Configure input\n",
    "            # real_imgs = Variable(imgs.type(FloatTensor))\n",
    "            # labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "            real_imgs = imgs\n",
    "            labels = LongTensor(labels)\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "            gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            validity, pred_label = discriminator(gen_imgs)\n",
    "            g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Loss for real images\n",
    "            real_pred, real_aux = discriminator(real_imgs)\n",
    "            d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "            # Loss for fake images\n",
    "            fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "            d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "            gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "            d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item())\n",
    "            )\n",
    "        # batches_done = epoch * len(dataloader) + i\n",
    "        # if batches_done % sample_interval == 0:\n",
    "        #     sample_image(n_row=10, batches_done=batches_done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_imgs = generator(z, gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = gen_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = im.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21518027430>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imm.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
